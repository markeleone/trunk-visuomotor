{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import csv\n",
    "import cv2\n",
    "import data_aug\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with trunk kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING DATA\n",
    "Trim dataset for single image set to one x3, y3, z3 set per image. This contains the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rb_pos_csv(input_csv, output_csv):\n",
    "    # Check if the output CSV exists and create it with the header if it doesn't\n",
    "    if not os.path.exists(output_csv):\n",
    "        header = ['ID', 'x1', 'y1', 'z1', 'x2', 'y2', 'z2', 'x3', 'y3', 'z3', 'isGripperOpen', 'img_filename']\n",
    "        with open(output_csv, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "\n",
    "    # Read the input CSV\n",
    "    positions_df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Initialize variables to keep track of the last image filename and the last row to write\n",
    "    last_img_filename = None\n",
    "    last_output_row = None\n",
    "\n",
    "    # Process each row in the input CSV\n",
    "    for index, row in positions_df.iterrows():\n",
    "        cur_img_filename = row['img_filename']\n",
    "        #output_row = [row['ID'], row['x3'], row['y3'], row['z3'], cur_img_filename] #for tip position\n",
    "        output_row = row\n",
    "\n",
    "        # If the current image filename is different from the last, write the last output row\n",
    "        if last_img_filename is not None and last_img_filename != cur_img_filename:\n",
    "            with open(output_csv, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(last_output_row) \n",
    "\n",
    "        # Update the last image filename and last output row\n",
    "        last_img_filename = cur_img_filename\n",
    "        last_output_row = output_row\n",
    "        #last_output_row_augmented = output_row_augmented\n",
    "\n",
    "    # Write the last row (for the final image in the sequence)\n",
    "    if last_output_row is not None:\n",
    "        with open(output_csv, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(last_output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/mocap_rb'\n",
    "input_csv = os.path.join(data_dir, 'raw/single_img_regression.csv')  # replace with your actual CSV filename\n",
    "output_csv = os.path.join(data_dir, 'single_img_regression_mocap_rb.csv')\n",
    "make_rb_pos_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot tip position overlaid on images to verify dataset quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pillow_coords(df, img_width, img_height, x_min, x_max, z_min, z_max):\n",
    "    \"\"\"\n",
    "    Convert robot coordinates to Pillow image coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'x' and 'z' columns in robot coordinates.\n",
    "    img_width, img_height (int): Dimensions of the Pillow image.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with 'img_x' and 'img_y' columns for Pillow image coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the scaling factors for x and z coordinates\n",
    "    x_scale = img_width / (x_max - x_min)\n",
    "    z_scale = img_height / (z_max - z_min)\n",
    "\n",
    "    # Calculate the shifts to center the robot's origin within the image\n",
    "    x_shift = (x_max + x_min) / 2\n",
    "    z_shift = (z_max + z_min) / 2\n",
    "\n",
    "    # Convert robot coordinates to Pillow image coordinates\n",
    "    df['img_x'] = (df['x'] - x_shift) * x_scale + img_width / 2\n",
    "    df['img_y'] = (df['z'] - z_shift) * z_scale + img_height / 2\n",
    "\n",
    "    # Invert the y-axis and x-axis to match Pillow's coordinate system (where (0, 0) is top-left)\n",
    "    df['img_y'] = img_height - df['img_y']\n",
    "    df['img_x'] = img_width - df['img_x']\n",
    "\n",
    "    return df[['img_x', 'img_y']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_on_images(results, data_dir, output_folder, dataset_file):\n",
    "    \"\"\"\n",
    "    Plot ground truth and predicted tip positions on images.\n",
    "\n",
    "    Parameters:\n",
    "    results (list): List of dictionaries containing img_filename, true_x, true_z, pred_x, pred_z\n",
    "    data_dir (str): Directory containing the images.\n",
    "    \"\"\"\n",
    "    image_folder = os.path.join(data_dir, 'raw/images')\n",
    "\n",
    "    for result in results:\n",
    "        image_filename = result['img_filename']\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "\n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "\n",
    "            # Convert ground truth and predicted coordinates to image coordinates\n",
    "            positions_df = pd.read_csv(dataset_file)\n",
    "\n",
    "\n",
    "            # Convert entire DataFrame coordinates to Pillow image coordinates once\n",
    "            overall_df = positions_df[['x3', 'z3']].rename(columns={'x3': 'x', 'z3': 'z'})\n",
    "\n",
    "            #calculate extent of dataset for scaling to pillow coords\n",
    "            x_min, x_max = overall_df['x'].min() - 0.01, overall_df['x'].max() + 0.01\n",
    "            z_min, z_max = overall_df['z'].min() - 0.01, overall_df['z'].max() + 0.01\n",
    "\n",
    "            true_img_coords = convert_to_pillow_coords(pd.DataFrame({'x': [result['true_x']], 'z': [result['true_z']]}), img_width, img_height, x_min, x_max, z_min, z_max)\n",
    "            pred_img_coords = convert_to_pillow_coords(pd.DataFrame({'x': [result['pred_x']], 'z': [result['pred_z']]}), img_width, img_height,  x_min, x_max, z_min, z_max)\n",
    "\n",
    "            # Create a figure with the same dimensions as the image\n",
    "            fig, ax = plt.subplots(figsize=(img_width / 100, img_height / 100), dpi=100)\n",
    "\n",
    "            # Plot the image\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Plot the ground truth tip position\n",
    "            ax.scatter([true_img_coords['img_x'][0]], [true_img_coords['img_y'][0]], color='green', s=200, label='Ground Truth')\n",
    "\n",
    "            # Plot the predicted tip position\n",
    "            ax.scatter([pred_img_coords['img_x'][0]], [pred_img_coords['img_y'][0]], color='red', s=200, label='Prediction')\n",
    "\n",
    "            # Remove axes for a cleaner output\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add a legend\n",
    "            ax.legend()\n",
    "\n",
    "            # Save the image with the original dimensions\n",
    "            output_filename = os.path.join(data_dir, f\"model_validation/{output_folder}/output_{image_filename}\")\n",
    "            plt.savefig(output_filename, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Plotted ground truth and prediction on {image_filename} and saved as {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new csvs for train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/mocap_rb'\n",
    "output_dir = 'data/mocap_rb/split'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_file = os.path.join(input_dir, 'single_img_regression_mocap_rb.csv')\n",
    "image_dir = os.path.join(input_dir, 'raw/images')\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Split the data (80-10-10)\n",
    "train_filenames, test_filenames = train_test_split(df['img_filename'].tolist(), test_size=0.1, random_state=42)\n",
    "train_filenames, val_filenames = train_test_split(train_filenames, test_size=0.12, random_state=42) # 0.12 * 0.9 = 0.1\n",
    "\n",
    "# Filter the original DataFrame to create train, validation, and test CSVs\n",
    "train_df = df[df['img_filename'].isin(train_filenames)]\n",
    "val_df = df[df['img_filename'].isin(val_filenames)]\n",
    "test_df = df[df['img_filename'].isin(test_filenames)]\n",
    "\n",
    "# Save the new CSVs\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "val_csv_path = os.path.join(output_dir, 'val.csv')\n",
    "test_csv_path = os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment images and update train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "test_dir = os.path.join(output_dir, 'images/test')\n",
    "val_dir = os.path.join(output_dir, 'images/val')\n",
    "train_dir = os.path.join(output_dir, \"images/train\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "importlib.reload(data_aug)\n",
    "\n",
    "# Augment the training images and update the CSV\n",
    "augmented_rows = []\n",
    "for filename in train_filenames:\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    augmented_filename = data_aug.augment_image(image_path, train_dir)\n",
    "    \n",
    "    # Add the new row for the augmented image to the augmented_rows list\n",
    "    new_row = train_df[train_df['img_filename'] == filename].copy()\n",
    "    new_row['img_filename'] = augmented_filename\n",
    "    augmented_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame for the augmented data and append to train_df\n",
    "augmented_df = pd.concat(augmented_rows)\n",
    "final_train_df = pd.concat([train_df, augmented_df])\n",
    "\n",
    "# Save the updated train CSV\n",
    "final_train_df.to_csv(train_csv_path, index=False)\n",
    "\n",
    "# Copy validation and test images and CSVs without augmentation\n",
    "for filename in val_filenames:\n",
    "    src_path = os.path.join(image_dir, filename)\n",
    "    data_aug.crop_and_resize(src_path, val_dir)\n",
    "\n",
    "for filename in test_filenames:\n",
    "    src_path = os.path.join(image_dir, filename)\n",
    "    data_aug.crop_and_resize(src_path, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx]['img_filename'])  # Use column name\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        labels = self.data_frame.iloc[idx, 1:10].values\n",
    "        labels = labels.astype('float').tolist()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image, labels, and the image filename\n",
    "        return image, torch.tensor(labels), self.data_frame.iloc[idx]['img_filename']  # Use column name\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # image input size to model (224,224)\n",
    "    # out of memory error with 1080x1080, batch size 32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Function to calculate RMSE\n",
    "criterion = nn.MSELoss()\n",
    "def calculate_rmse(outputs, labels):\n",
    "    mse_loss = criterion(outputs, labels)\n",
    "    return torch.sqrt(mse_loss).item()\n",
    "\n",
    "# Dataset parameters\n",
    "data_dir = 'data/mocap_rb/split' #just change this when you change datasets\n",
    "\n",
    "train_csv_file = os.path.join(data_dir, 'train.csv')\n",
    "train_img_dir = os.path.join(data_dir, 'images/train')\n",
    "\n",
    "val_csv_file = os.path.join(data_dir, 'val.csv')\n",
    "val_img_dir = os.path.join(data_dir, 'images/val')\n",
    "\n",
    "test_csv_file = os.path.join(data_dir, 'test.csv')\n",
    "test_img_dir = os.path.join(data_dir, 'images/test')\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageDataset(csv_file=train_csv_file, root_dir=train_img_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = ImageDataset(csv_file=val_csv_file, root_dir=val_img_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = ImageDataset(csv_file=test_csv_file, root_dir=test_img_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA available (should be on ceres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markleone/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/markleone/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the last fully connected layer to output 9 values (x1, y1, z1, x2, y2, z2, x3, y3, z3)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 9) #output layers is 9-vector\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.1924, Train RMSE: 0.3981, Val RMSE: 2.0559\n",
      "Epoch [2/200], Loss: 0.0039, Train RMSE: 0.0610, Val RMSE: 1.6595\n",
      "Epoch [3/200], Loss: 0.0018, Train RMSE: 0.0424, Val RMSE: 0.9954\n",
      "Epoch [4/200], Loss: 0.0012, Train RMSE: 0.0347, Val RMSE: 0.6009\n",
      "Epoch [5/200], Loss: 0.0007, Train RMSE: 0.0264, Val RMSE: 0.3743\n",
      "Epoch [6/200], Loss: 0.0006, Train RMSE: 0.0252, Val RMSE: 0.2103\n",
      "Epoch [7/200], Loss: 0.0007, Train RMSE: 0.0257, Val RMSE: 0.1004\n",
      "Epoch [8/200], Loss: 0.0004, Train RMSE: 0.0195, Val RMSE: 0.0706\n",
      "Epoch [9/200], Loss: 0.0007, Train RMSE: 0.0250, Val RMSE: 0.0544\n",
      "Epoch [10/200], Loss: 0.0005, Train RMSE: 0.0224, Val RMSE: 0.0326\n",
      "Epoch [11/200], Loss: 0.0004, Train RMSE: 0.0183, Val RMSE: 0.0322\n",
      "Epoch [12/200], Loss: 0.0005, Train RMSE: 0.0222, Val RMSE: 0.0261\n",
      "Epoch [13/200], Loss: 0.0004, Train RMSE: 0.0204, Val RMSE: 0.0227\n",
      "Epoch [14/200], Loss: 0.0004, Train RMSE: 0.0194, Val RMSE: 0.0163\n",
      "Epoch [15/200], Loss: 0.0003, Train RMSE: 0.0171, Val RMSE: 0.0279\n",
      "Epoch [16/200], Loss: 0.0006, Train RMSE: 0.0238, Val RMSE: 0.0299\n",
      "Epoch [17/200], Loss: 0.0006, Train RMSE: 0.0232, Val RMSE: 0.0153\n",
      "Epoch [18/200], Loss: 0.0004, Train RMSE: 0.0185, Val RMSE: 0.0192\n",
      "Epoch [19/200], Loss: 0.0004, Train RMSE: 0.0195, Val RMSE: 0.0172\n",
      "Epoch [20/200], Loss: 0.0004, Train RMSE: 0.0185, Val RMSE: 0.0197\n",
      "Epoch [21/200], Loss: 0.0004, Train RMSE: 0.0195, Val RMSE: 0.0137\n",
      "Epoch [22/200], Loss: 0.0003, Train RMSE: 0.0160, Val RMSE: 0.0127\n",
      "Epoch [23/200], Loss: 0.0002, Train RMSE: 0.0130, Val RMSE: 0.0165\n",
      "Epoch [24/200], Loss: 0.0003, Train RMSE: 0.0173, Val RMSE: 0.0181\n",
      "Epoch [25/200], Loss: 0.0003, Train RMSE: 0.0181, Val RMSE: 0.0222\n",
      "Epoch [26/200], Loss: 0.0006, Train RMSE: 0.0235, Val RMSE: 0.0256\n",
      "Epoch [27/200], Loss: 0.0009, Train RMSE: 0.0296, Val RMSE: 0.0181\n",
      "Epoch [28/200], Loss: 0.0005, Train RMSE: 0.0231, Val RMSE: 0.0257\n",
      "Epoch [29/200], Loss: 0.0005, Train RMSE: 0.0208, Val RMSE: 0.0273\n",
      "Epoch [30/200], Loss: 0.0005, Train RMSE: 0.0213, Val RMSE: 0.0192\n",
      "Epoch [31/200], Loss: 0.0003, Train RMSE: 0.0159, Val RMSE: 0.0133\n",
      "Epoch [32/200], Loss: 0.0002, Train RMSE: 0.0136, Val RMSE: 0.0114\n",
      "Epoch [33/200], Loss: 0.0003, Train RMSE: 0.0163, Val RMSE: 0.0258\n",
      "Epoch [34/200], Loss: 0.0005, Train RMSE: 0.0218, Val RMSE: 0.0191\n",
      "Epoch [35/200], Loss: 0.0005, Train RMSE: 0.0214, Val RMSE: 0.0153\n",
      "Epoch [36/200], Loss: 0.0005, Train RMSE: 0.0209, Val RMSE: 0.0142\n",
      "Epoch [37/200], Loss: 0.0003, Train RMSE: 0.0178, Val RMSE: 0.0134\n",
      "Epoch [38/200], Loss: 0.0003, Train RMSE: 0.0159, Val RMSE: 0.0313\n",
      "Epoch [39/200], Loss: 0.0005, Train RMSE: 0.0209, Val RMSE: 0.0318\n",
      "Epoch [40/200], Loss: 0.0009, Train RMSE: 0.0283, Val RMSE: 0.0232\n",
      "Epoch [41/200], Loss: 0.0005, Train RMSE: 0.0222, Val RMSE: 0.0200\n",
      "Epoch [42/200], Loss: 0.0005, Train RMSE: 0.0221, Val RMSE: 0.0130\n",
      "Epoch [43/200], Loss: 0.0002, Train RMSE: 0.0131, Val RMSE: 0.0164\n",
      "Epoch [44/200], Loss: 0.0001, Train RMSE: 0.0119, Val RMSE: 0.0183\n",
      "Epoch [45/200], Loss: 0.0003, Train RMSE: 0.0161, Val RMSE: 0.0270\n",
      "Epoch [46/200], Loss: 0.0006, Train RMSE: 0.0238, Val RMSE: 0.0173\n",
      "Epoch [47/200], Loss: 0.0004, Train RMSE: 0.0204, Val RMSE: 0.0163\n",
      "Epoch [48/200], Loss: 0.0003, Train RMSE: 0.0167, Val RMSE: 0.0152\n",
      "Epoch [49/200], Loss: 0.0002, Train RMSE: 0.0140, Val RMSE: 0.0251\n",
      "Epoch [50/200], Loss: 0.0004, Train RMSE: 0.0184, Val RMSE: 0.0187\n",
      "Epoch [51/200], Loss: 0.0003, Train RMSE: 0.0171, Val RMSE: 0.0170\n",
      "Epoch [52/200], Loss: 0.0003, Train RMSE: 0.0165, Val RMSE: 0.0116\n",
      "Epoch [53/200], Loss: 0.0003, Train RMSE: 0.0178, Val RMSE: 0.0207\n",
      "Epoch [54/200], Loss: 0.0005, Train RMSE: 0.0207, Val RMSE: 0.0212\n",
      "Epoch [55/200], Loss: 0.0004, Train RMSE: 0.0198, Val RMSE: 0.0313\n",
      "Epoch [56/200], Loss: 0.0009, Train RMSE: 0.0287, Val RMSE: 0.0195\n",
      "Epoch [57/200], Loss: 0.0004, Train RMSE: 0.0203, Val RMSE: 0.0208\n",
      "Epoch [58/200], Loss: 0.0005, Train RMSE: 0.0216, Val RMSE: 0.0207\n",
      "Epoch [59/200], Loss: 0.0004, Train RMSE: 0.0193, Val RMSE: 0.0148\n",
      "Epoch [60/200], Loss: 0.0002, Train RMSE: 0.0141, Val RMSE: 0.0216\n",
      "Epoch [61/200], Loss: 0.0004, Train RMSE: 0.0191, Val RMSE: 0.0170\n",
      "Epoch [62/200], Loss: 0.0002, Train RMSE: 0.0128, Val RMSE: 0.0119\n",
      "Epoch [63/200], Loss: 0.0002, Train RMSE: 0.0133, Val RMSE: 0.0163\n",
      "Epoch [64/200], Loss: 0.0002, Train RMSE: 0.0135, Val RMSE: 0.0286\n",
      "Epoch [65/200], Loss: 0.0008, Train RMSE: 0.0279, Val RMSE: 0.0302\n",
      "Epoch [66/200], Loss: 0.0008, Train RMSE: 0.0281, Val RMSE: 0.0234\n",
      "Epoch [67/200], Loss: 0.0007, Train RMSE: 0.0256, Val RMSE: 0.0178\n",
      "Epoch [68/200], Loss: 0.0006, Train RMSE: 0.0239, Val RMSE: 0.0259\n",
      "Epoch [69/200], Loss: 0.0006, Train RMSE: 0.0232, Val RMSE: 0.0175\n",
      "Epoch [70/200], Loss: 0.0005, Train RMSE: 0.0195, Val RMSE: 0.0155\n",
      "Epoch [71/200], Loss: 0.0004, Train RMSE: 0.0191, Val RMSE: 0.0203\n",
      "Epoch [72/200], Loss: 0.0004, Train RMSE: 0.0188, Val RMSE: 0.0284\n",
      "Epoch [73/200], Loss: 0.0007, Train RMSE: 0.0253, Val RMSE: 0.0360\n",
      "Epoch [74/200], Loss: 0.0010, Train RMSE: 0.0293, Val RMSE: 0.0235\n",
      "Epoch [75/200], Loss: 0.0005, Train RMSE: 0.0226, Val RMSE: 0.0300\n",
      "Epoch [76/200], Loss: 0.0007, Train RMSE: 0.0253, Val RMSE: 0.0297\n",
      "Epoch [77/200], Loss: 0.0009, Train RMSE: 0.0294, Val RMSE: 0.0216\n",
      "Epoch [78/200], Loss: 0.0012, Train RMSE: 0.0328, Val RMSE: 0.0232\n",
      "Epoch [79/200], Loss: 0.0011, Train RMSE: 0.0307, Val RMSE: 0.0111\n",
      "Epoch [80/200], Loss: 0.0005, Train RMSE: 0.0214, Val RMSE: 0.0128\n",
      "Epoch [81/200], Loss: 0.0005, Train RMSE: 0.0216, Val RMSE: 0.0107\n",
      "Epoch [82/200], Loss: 0.0003, Train RMSE: 0.0164, Val RMSE: 0.0170\n",
      "Epoch [83/200], Loss: 0.0002, Train RMSE: 0.0147, Val RMSE: 0.0181\n",
      "Epoch [84/200], Loss: 0.0002, Train RMSE: 0.0147, Val RMSE: 0.0147\n",
      "Epoch [85/200], Loss: 0.0002, Train RMSE: 0.0146, Val RMSE: 0.0156\n",
      "Epoch [86/200], Loss: 0.0002, Train RMSE: 0.0122, Val RMSE: 0.0187\n",
      "Epoch [87/200], Loss: 0.0004, Train RMSE: 0.0188, Val RMSE: 0.0178\n",
      "Epoch [88/200], Loss: 0.0005, Train RMSE: 0.0223, Val RMSE: 0.0215\n",
      "Epoch [89/200], Loss: 0.0008, Train RMSE: 0.0276, Val RMSE: 0.0205\n",
      "Epoch [90/200], Loss: 0.0005, Train RMSE: 0.0228, Val RMSE: 0.0118\n",
      "Epoch [91/200], Loss: 0.0003, Train RMSE: 0.0160, Val RMSE: 0.0155\n",
      "Epoch [92/200], Loss: 0.0005, Train RMSE: 0.0214, Val RMSE: 0.0301\n",
      "Epoch [93/200], Loss: 0.0006, Train RMSE: 0.0235, Val RMSE: 0.0309\n",
      "Epoch [94/200], Loss: 0.0007, Train RMSE: 0.0261, Val RMSE: 0.0198\n",
      "Epoch [95/200], Loss: 0.0008, Train RMSE: 0.0274, Val RMSE: 0.0132\n",
      "Epoch [96/200], Loss: 0.0005, Train RMSE: 0.0206, Val RMSE: 0.0216\n",
      "Epoch [97/200], Loss: 0.0004, Train RMSE: 0.0201, Val RMSE: 0.0268\n",
      "Epoch [98/200], Loss: 0.0004, Train RMSE: 0.0179, Val RMSE: 0.0139\n",
      "Epoch [99/200], Loss: 0.0002, Train RMSE: 0.0153, Val RMSE: 0.0146\n",
      "Epoch [100/200], Loss: 0.0002, Train RMSE: 0.0138, Val RMSE: 0.0122\n",
      "Epoch [101/200], Loss: 0.0002, Train RMSE: 0.0133, Val RMSE: 0.0107\n",
      "Epoch [102/200], Loss: 0.0001, Train RMSE: 0.0114, Val RMSE: 0.0117\n",
      "Epoch [103/200], Loss: 0.0002, Train RMSE: 0.0121, Val RMSE: 0.0114\n",
      "Epoch [104/200], Loss: 0.0002, Train RMSE: 0.0142, Val RMSE: 0.0186\n",
      "Epoch [105/200], Loss: 0.0003, Train RMSE: 0.0164, Val RMSE: 0.0126\n",
      "Epoch [106/200], Loss: 0.0004, Train RMSE: 0.0179, Val RMSE: 0.0137\n",
      "Epoch [107/200], Loss: 0.0008, Train RMSE: 0.0278, Val RMSE: 0.0159\n",
      "Epoch [108/200], Loss: 0.0005, Train RMSE: 0.0205, Val RMSE: 0.0213\n",
      "Epoch [109/200], Loss: 0.0004, Train RMSE: 0.0178, Val RMSE: 0.0319\n",
      "Epoch [110/200], Loss: 0.0006, Train RMSE: 0.0235, Val RMSE: 0.0252\n",
      "Epoch [111/200], Loss: 0.0005, Train RMSE: 0.0213, Val RMSE: 0.0200\n",
      "Epoch [112/200], Loss: 0.0005, Train RMSE: 0.0226, Val RMSE: 0.0211\n",
      "Epoch [113/200], Loss: 0.0006, Train RMSE: 0.0225, Val RMSE: 0.0220\n",
      "Epoch [114/200], Loss: 0.0006, Train RMSE: 0.0237, Val RMSE: 0.0267\n",
      "Epoch [115/200], Loss: 0.0005, Train RMSE: 0.0221, Val RMSE: 0.0263\n",
      "Epoch [116/200], Loss: 0.0005, Train RMSE: 0.0223, Val RMSE: 0.0157\n",
      "Epoch [117/200], Loss: 0.0004, Train RMSE: 0.0195, Val RMSE: 0.0278\n",
      "Epoch [118/200], Loss: 0.0005, Train RMSE: 0.0221, Val RMSE: 0.0219\n",
      "Epoch [119/200], Loss: 0.0005, Train RMSE: 0.0205, Val RMSE: 0.0133\n",
      "Epoch [120/200], Loss: 0.0003, Train RMSE: 0.0167, Val RMSE: 0.0220\n",
      "Epoch [121/200], Loss: 0.0003, Train RMSE: 0.0161, Val RMSE: 0.0217\n",
      "Epoch [122/200], Loss: 0.0004, Train RMSE: 0.0193, Val RMSE: 0.0116\n",
      "Epoch [123/200], Loss: 0.0004, Train RMSE: 0.0192, Val RMSE: 0.0128\n",
      "Epoch [124/200], Loss: 0.0003, Train RMSE: 0.0157, Val RMSE: 0.0117\n",
      "Epoch [125/200], Loss: 0.0001, Train RMSE: 0.0117, Val RMSE: 0.0159\n",
      "Epoch [126/200], Loss: 0.0002, Train RMSE: 0.0151, Val RMSE: 0.0188\n",
      "Epoch [127/200], Loss: 0.0003, Train RMSE: 0.0172, Val RMSE: 0.0164\n",
      "Epoch [128/200], Loss: 0.0003, Train RMSE: 0.0156, Val RMSE: 0.0138\n",
      "Epoch [129/200], Loss: 0.0002, Train RMSE: 0.0123, Val RMSE: 0.0115\n",
      "Epoch [130/200], Loss: 0.0001, Train RMSE: 0.0100, Val RMSE: 0.0099\n",
      "Epoch [131/200], Loss: 0.0001, Train RMSE: 0.0104, Val RMSE: 0.0158\n",
      "Epoch [132/200], Loss: 0.0002, Train RMSE: 0.0152, Val RMSE: 0.0149\n",
      "Epoch [133/200], Loss: 0.0003, Train RMSE: 0.0168, Val RMSE: 0.0158\n",
      "Epoch [134/200], Loss: 0.0003, Train RMSE: 0.0166, Val RMSE: 0.0171\n",
      "Epoch [135/200], Loss: 0.0003, Train RMSE: 0.0183, Val RMSE: 0.0144\n",
      "Epoch [136/200], Loss: 0.0002, Train RMSE: 0.0143, Val RMSE: 0.0147\n",
      "Epoch [137/200], Loss: 0.0002, Train RMSE: 0.0125, Val RMSE: 0.0170\n",
      "Epoch [138/200], Loss: 0.0002, Train RMSE: 0.0133, Val RMSE: 0.0241\n",
      "Epoch [139/200], Loss: 0.0005, Train RMSE: 0.0220, Val RMSE: 0.0196\n",
      "Epoch [140/200], Loss: 0.0004, Train RMSE: 0.0192, Val RMSE: 0.0319\n",
      "Epoch [141/200], Loss: 0.0006, Train RMSE: 0.0233, Val RMSE: 0.0201\n",
      "Epoch [142/200], Loss: 0.0004, Train RMSE: 0.0188, Val RMSE: 0.0226\n",
      "Epoch [143/200], Loss: 0.0004, Train RMSE: 0.0196, Val RMSE: 0.0196\n",
      "Epoch [144/200], Loss: 0.0002, Train RMSE: 0.0133, Val RMSE: 0.0167\n",
      "Epoch [145/200], Loss: 0.0002, Train RMSE: 0.0148, Val RMSE: 0.0125\n",
      "Epoch [146/200], Loss: 0.0002, Train RMSE: 0.0144, Val RMSE: 0.0215\n",
      "Epoch [147/200], Loss: 0.0007, Train RMSE: 0.0252, Val RMSE: 0.0179\n",
      "Epoch [148/200], Loss: 0.0003, Train RMSE: 0.0181, Val RMSE: 0.0203\n",
      "Epoch [149/200], Loss: 0.0003, Train RMSE: 0.0181, Val RMSE: 0.0219\n",
      "Epoch [150/200], Loss: 0.0005, Train RMSE: 0.0217, Val RMSE: 0.0230\n",
      "Epoch [151/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0172\n",
      "Epoch [152/200], Loss: 0.0005, Train RMSE: 0.0209, Val RMSE: 0.0143\n",
      "Epoch [153/200], Loss: 0.0005, Train RMSE: 0.0207, Val RMSE: 0.0331\n",
      "Epoch [154/200], Loss: 0.0009, Train RMSE: 0.0286, Val RMSE: 0.0352\n",
      "Epoch [155/200], Loss: 0.0011, Train RMSE: 0.0311, Val RMSE: 0.0220\n",
      "Epoch [156/200], Loss: 0.0003, Train RMSE: 0.0176, Val RMSE: 0.0304\n",
      "Epoch [157/200], Loss: 0.0008, Train RMSE: 0.0267, Val RMSE: 0.0259\n",
      "Epoch [158/200], Loss: 0.0005, Train RMSE: 0.0213, Val RMSE: 0.0129\n",
      "Epoch [159/200], Loss: 0.0001, Train RMSE: 0.0114, Val RMSE: 0.0138\n",
      "Epoch [160/200], Loss: 0.0003, Train RMSE: 0.0159, Val RMSE: 0.0175\n",
      "Epoch [161/200], Loss: 0.0003, Train RMSE: 0.0163, Val RMSE: 0.0194\n",
      "Epoch [162/200], Loss: 0.0004, Train RMSE: 0.0194, Val RMSE: 0.0178\n",
      "Epoch [163/200], Loss: 0.0004, Train RMSE: 0.0192, Val RMSE: 0.0179\n",
      "Epoch [164/200], Loss: 0.0003, Train RMSE: 0.0174, Val RMSE: 0.0214\n",
      "Epoch [165/200], Loss: 0.0004, Train RMSE: 0.0190, Val RMSE: 0.0269\n",
      "Epoch [166/200], Loss: 0.0005, Train RMSE: 0.0219, Val RMSE: 0.0209\n",
      "Epoch [167/200], Loss: 0.0005, Train RMSE: 0.0215, Val RMSE: 0.0166\n",
      "Epoch [168/200], Loss: 0.0006, Train RMSE: 0.0238, Val RMSE: 0.0232\n",
      "Epoch [169/200], Loss: 0.0008, Train RMSE: 0.0271, Val RMSE: 0.0223\n",
      "Epoch [170/200], Loss: 0.0005, Train RMSE: 0.0217, Val RMSE: 0.0229\n",
      "Epoch [171/200], Loss: 0.0008, Train RMSE: 0.0273, Val RMSE: 0.0182\n",
      "Epoch [172/200], Loss: 0.0005, Train RMSE: 0.0217, Val RMSE: 0.0225\n",
      "Epoch [173/200], Loss: 0.0007, Train RMSE: 0.0259, Val RMSE: 0.0308\n",
      "Epoch [174/200], Loss: 0.0006, Train RMSE: 0.0235, Val RMSE: 0.0307\n",
      "Epoch [175/200], Loss: 0.0010, Train RMSE: 0.0303, Val RMSE: 0.0254\n",
      "Epoch [176/200], Loss: 0.0006, Train RMSE: 0.0250, Val RMSE: 0.0225\n",
      "Epoch [177/200], Loss: 0.0005, Train RMSE: 0.0211, Val RMSE: 0.0190\n",
      "Epoch [178/200], Loss: 0.0002, Train RMSE: 0.0143, Val RMSE: 0.0138\n",
      "Epoch [179/200], Loss: 0.0002, Train RMSE: 0.0138, Val RMSE: 0.0098\n",
      "Epoch [180/200], Loss: 0.0004, Train RMSE: 0.0183, Val RMSE: 0.0158\n",
      "Epoch [181/200], Loss: 0.0005, Train RMSE: 0.0211, Val RMSE: 0.0143\n",
      "Epoch [182/200], Loss: 0.0004, Train RMSE: 0.0169, Val RMSE: 0.0120\n",
      "Epoch [183/200], Loss: 0.0005, Train RMSE: 0.0194, Val RMSE: 0.0142\n",
      "Epoch [184/200], Loss: 0.0006, Train RMSE: 0.0239, Val RMSE: 0.0139\n",
      "Epoch [185/200], Loss: 0.0007, Train RMSE: 0.0255, Val RMSE: 0.0187\n",
      "Epoch [186/200], Loss: 0.0008, Train RMSE: 0.0272, Val RMSE: 0.0274\n",
      "Epoch [187/200], Loss: 0.0007, Train RMSE: 0.0268, Val RMSE: 0.0253\n",
      "Epoch [188/200], Loss: 0.0005, Train RMSE: 0.0221, Val RMSE: 0.0241\n",
      "Epoch [189/200], Loss: 0.0004, Train RMSE: 0.0196, Val RMSE: 0.0157\n",
      "Epoch [190/200], Loss: 0.0002, Train RMSE: 0.0131, Val RMSE: 0.0197\n",
      "Epoch [191/200], Loss: 0.0003, Train RMSE: 0.0159, Val RMSE: 0.0175\n",
      "Epoch [192/200], Loss: 0.0002, Train RMSE: 0.0141, Val RMSE: 0.0128\n",
      "Epoch [193/200], Loss: 0.0003, Train RMSE: 0.0174, Val RMSE: 0.0206\n",
      "Epoch [194/200], Loss: 0.0003, Train RMSE: 0.0162, Val RMSE: 0.0161\n",
      "Epoch [195/200], Loss: 0.0002, Train RMSE: 0.0133, Val RMSE: 0.0129\n",
      "Epoch [196/200], Loss: 0.0003, Train RMSE: 0.0170, Val RMSE: 0.0145\n",
      "Epoch [197/200], Loss: 0.0003, Train RMSE: 0.0169, Val RMSE: 0.0131\n",
      "Epoch [198/200], Loss: 0.0002, Train RMSE: 0.0125, Val RMSE: 0.0132\n",
      "Epoch [199/200], Loss: 0.0002, Train RMSE: 0.0132, Val RMSE: 0.0126\n",
      "Epoch [200/200], Loss: 0.0002, Train RMSE: 0.0139, Val RMSE: 0.0150\n",
      "Finished Training\n",
      "Best model saved with Val RMSE: 0.0098\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define checkpoint path\n",
    "checkpoint_dir = os.path.join(data_dir, 'checkpoint')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with accuracy tracking and checkpoint saving\n",
    "num_epochs = 200\n",
    "checkpoint_interval = 50  # Save checkpoint every 'n' epochs\n",
    "\n",
    "train_rmse_history = []\n",
    "val_rmse_history = []\n",
    "\n",
    "best_val_rmse = float('inf')  # Initialize best validation RMSE to a large value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_rmse = 0.0\n",
    "    for inputs, labels, _ in train_loader:  # Assuming train_loader is your training DataLoader\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate RMSE for this batch\n",
    "        train_rmse += calculate_rmse(outputs, labels)\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_rmse /= len(train_loader)  # Average RMSE for the epoch\n",
    "    train_rmse_history.append(train_rmse)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_rmse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_rmse += calculate_rmse(outputs, labels)\n",
    "\n",
    "    val_rmse /= len(val_loader)  # Average RMSE for the epoch\n",
    "    val_rmse_history.append(val_rmse)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}')\n",
    "\n",
    "    # Save checkpoint if it is a checkpoint interval or the last epoch\n",
    "    if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        checkpoint_file = f'model_checkpoint_epoch_{epoch+1}.pth'\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        #print(f'Saved checkpoint: {checkpoint_path}')\n",
    "\n",
    "    # Track the best validation RMSE and save the best model\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        #print(f'Best model saved with Val RMSE: {best_val_rmse:.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "print(f'Best model saved with Val RMSE: {best_val_rmse:.4f}')\n",
    "\n",
    "# Plot training and validation RMSE over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_rmse_history, label='Train RMSE')\n",
    "plt.plot(range(1, num_epochs + 1), val_rmse_history, label='Val RMSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Train and Val RMSE Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# for 224x224 3-vector/9-vector and 30-img dataset, 200 epochs (10x faster on gpu than my mac, 5 min on GPU)...  barely depends on output dimension\n",
    "# for 224x224 9-vector and 90-img dataset, each epoch takes appx. 4 seconds. 4*200/60 = 13min to train on gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate the model on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[ 0.09982684 -0.10590546  0.10914638  0.12180062 -0.1908819   0.09507809\n",
      "   0.18991438 -0.2681651   0.07661898]], Ground Truth: [[ 0.10162247 -0.1021838   0.10157967  0.12380727 -0.19118585  0.09285767\n",
      "   0.19168857 -0.27090502  0.07665829]]\n",
      "Predicted: [[ 0.09634131 -0.10556635  0.11279933  0.10227039 -0.19469616  0.10742121\n",
      "   0.10904787 -0.30465746  0.0935568 ]], Ground Truth: [[ 0.09752542 -0.10782228  0.10791918  0.10317829 -0.20394564  0.11484016\n",
      "   0.12418049 -0.31882417  0.11297573]]\n",
      "Predicted: [[ 0.09589683 -0.10746521  0.11463551  0.09960893 -0.19602083  0.1034058\n",
      "   0.10849382 -0.30519387  0.09380519]], Ground Truth: [[ 0.09950597 -0.10897908  0.10655815  0.09826473 -0.2070977   0.11551723\n",
      "   0.08743949 -0.32567894  0.11862902]]\n",
      "Predicted: [[ 0.09515685 -0.10366276  0.11396043  0.07952787 -0.19219193  0.10893437\n",
      "   0.02543954 -0.2770031   0.10206742]], Ground Truth: [[ 0.09127886 -0.10330041  0.10374299  0.06112993 -0.18829069  0.10533468\n",
      "  -0.00670522 -0.27412128  0.0964485 ]]\n",
      "Predicted: [[ 9.40196663e-02 -1.02542862e-01  1.10191286e-01  1.01523355e-01\n",
      "  -1.84207812e-01  7.73613155e-02  1.16440758e-01 -2.64676273e-01\n",
      "   2.11110339e-04]], Ground Truth: [[ 0.09974821 -0.10125976  0.10332645  0.09915868 -0.19042024  0.08214099\n",
      "   0.1113518  -0.26445138  0.00599651]]\n",
      "Predicted: [[ 0.08973426 -0.10081619  0.11213836  0.07765153 -0.18551983  0.08959726\n",
      "   0.0335662  -0.2698576   0.03243505]], Ground Truth: [[ 0.09178334 -0.10299841  0.10419021  0.06486394 -0.18880048  0.10018196\n",
      "   0.01750158 -0.27343166  0.05145513]]\n",
      "Predicted: [[ 0.09924516 -0.10226171  0.11283764  0.12992513 -0.19483767  0.08956762\n",
      "   0.19883819 -0.2774067   0.03910204]], Ground Truth: [[ 0.1012328  -0.1009815   0.10470503  0.11953253 -0.1905277   0.09398725\n",
      "   0.16683729 -0.26485792  0.03409116]]\n",
      "Predicted: [[ 0.10255201 -0.10375549  0.11314463  0.12669384 -0.18660943  0.11655129\n",
      "   0.18732391 -0.26742655  0.12928805]], Ground Truth: [[ 0.10238683 -0.10039995  0.10285372  0.12683478 -0.18823196  0.1034926\n",
      "   0.19569667 -0.2653364   0.11538401]]\n",
      "Predicted: [[ 0.09590618 -0.10610974  0.11292963  0.07660352 -0.19022842  0.09176569\n",
      "   0.02532566 -0.2759242   0.04951299]], Ground Truth: [[ 0.09703157 -0.1043786   0.10390565  0.08065777 -0.19369602  0.10164175\n",
      "   0.03318304 -0.2849525   0.06614785]]\n",
      "Predicted: [[ 0.10040045 -0.10682458  0.10880268  0.12472901 -0.19130044  0.08706943\n",
      "   0.18268223 -0.27409187  0.03898392]], Ground Truth: [[ 0.10053162 -0.10251383  0.10329648  0.1180044  -0.19255301  0.09198596\n",
      "   0.1668809  -0.27544373  0.0440643 ]]\n",
      "Val RMSE = 0.009186391765251756\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(data_dir, 'checkpoint')\n",
    "best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# testin on training set (should be able to memorize training data as a first step)\n",
    "val_rmse = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):  # Test on the first 5 images\n",
    "        inputs, labels, _ = val_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_rmse += calculate_rmse(outputs, labels)\n",
    "        print(f'Predicted: {outputs.cpu().numpy()}, Ground Truth: {labels.cpu().numpy()}')\n",
    "val_rmse /= len(val_dataset)\n",
    "print(f'Val RMSE = {val_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[ 0.10201785 -0.10612787  0.11056413  0.12005121 -0.18975054  0.09365537\n",
      "   0.18063334 -0.2685684   0.079746  ]], Ground Truth: [[ 0.10179525 -0.10189842  0.10316835  0.1265398  -0.19032042  0.10217557\n",
      "   0.19473937 -0.27071783  0.08740528]]\n",
      "Predicted: [[ 0.09722351 -0.10779551  0.11280734  0.10128497 -0.19547538  0.1020373\n",
      "   0.10531555 -0.3055851   0.09248903]], Ground Truth: [[ 0.09978039 -0.10700434  0.10429376  0.10775504 -0.20411013  0.10470774\n",
      "   0.12960763 -0.3188564   0.09695425]]\n",
      "Predicted: [[ 0.09687789 -0.10679036  0.11686622  0.10244712 -0.19374551  0.1122421\n",
      "   0.11008936 -0.30893657  0.09914093]], Ground Truth: [[ 0.09621392 -0.10750578  0.10539389  0.09554794 -0.20447995  0.11144147\n",
      "   0.10266948 -0.32272992  0.09998971]]\n",
      "Predicted: [[ 0.09325506 -0.10227203  0.11583351  0.09768945 -0.1920826   0.10813486\n",
      "   0.10184452 -0.30410177  0.10333433]], Ground Truth: [[ 0.09936263 -0.10861763  0.10742743  0.09801421 -0.20636733  0.11554233\n",
      "   0.10223015 -0.3232491   0.11889925]]\n",
      "Predicted: [[ 0.09539688 -0.10414021  0.11328674  0.07675518 -0.19183506  0.10620957\n",
      "   0.02393325 -0.27385002  0.09703477]], Ground Truth: [[ 0.09184588 -0.10328438  0.10511728  0.06395073 -0.18904066  0.11002019\n",
      "  -0.00232039 -0.27674928  0.10276958]]\n",
      "Predicted: [[ 0.09969282 -0.10414019  0.1147294   0.12564683 -0.1893797   0.09435987\n",
      "   0.18974313 -0.27116823  0.04024739]], Ground Truth: [[ 0.09916356 -0.1019092   0.10474022  0.11173972 -0.19312839  0.0909068\n",
      "   0.1592442  -0.27018732  0.03555695]]\n",
      "Predicted: [[ 0.10188629 -0.1056143   0.1052218   0.12580428 -0.19130938  0.08325955\n",
      "   0.18290666 -0.26396057  0.02375001]], Ground Truth: [[ 0.10135566 -0.10221396  0.10085587  0.11737641 -0.19171661  0.08602143\n",
      "   0.16265598 -0.26932862  0.0292853 ]]\n",
      "Predicted: [[ 0.10341705 -0.10369426  0.1143079   0.12962611 -0.19001111  0.1143853\n",
      "   0.1868153  -0.2733265   0.13554393]], Ground Truth: [[ 0.10231663 -0.10200012  0.10366107  0.12220415 -0.19094041  0.10765053\n",
      "   0.18125394 -0.2767016   0.11939456]]\n",
      "Predicted: [[ 0.09379712 -0.10574223  0.11688637  0.0771807  -0.19091077  0.10850907\n",
      "   0.0257975  -0.28840372  0.11634689]], Ground Truth: [[ 0.09321241 -0.10312704  0.10595161  0.06739184 -0.19033223  0.11030916\n",
      "   0.01304115 -0.2836923   0.12719454]]\n",
      "Test RMSE = 0.009075968991965055\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# testin on training set (should be able to memorize training data as a first step)\n",
    "test_rmse = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):  # Test on the first 5 images\n",
    "        inputs, labels, _ = test_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_rmse += calculate_rmse(outputs, labels)\n",
    "        print(f'Predicted: {outputs.cpu().numpy()}, Ground Truth: {labels.cpu().numpy()}')\n",
    "test_rmse /= len(test_dataset)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display gt vs predicted tip positions on image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'img_filename': 'sample_2.jpg', 'true_x': 0.19168856739997864, 'true_z': 0.07665829360485077, 'pred_x': 0.18991437554359436, 'pred_z': 0.0766189768910408}, {'img_filename': 'sample_9.jpg', 'true_x': 0.12418048828840256, 'true_z': 0.11297573149204254, 'pred_x': 0.10904787480831146, 'pred_z': 0.09355679899454117}, {'img_filename': 'sample_21.jpg', 'true_x': 0.08743949234485626, 'true_z': 0.11862902343273163, 'pred_x': 0.10849381983280182, 'pred_z': 0.09380518645048141}, {'img_filename': 'sample_24.jpg', 'true_x': -0.0067052156664431095, 'true_z': 0.09644850343465805, 'pred_x': 0.025439541786909103, 'pred_z': 0.10206741839647293}, {'img_filename': 'sample_38.jpg', 'true_x': 0.11135179549455643, 'true_z': 0.005996509455144405, 'pred_x': 0.11644075810909271, 'pred_z': 0.00021111033856868744}, {'img_filename': 'sample_43.jpg', 'true_x': 0.017501583322882652, 'true_z': 0.05145512893795967, 'pred_x': 0.0335661955177784, 'pred_z': 0.03243505209684372}, {'img_filename': 'sample_51.jpg', 'true_x': 0.16683728992938995, 'true_z': 0.03409116342663765, 'pred_x': 0.19883818924427032, 'pred_z': 0.039102040231227875}, {'img_filename': 'sample_60.jpg', 'true_x': 0.1956966668367386, 'true_z': 0.11538400501012802, 'pred_x': 0.1873239129781723, 'pred_z': 0.12928804755210876}, {'img_filename': 'sample_79.jpg', 'true_x': 0.03318304196000099, 'true_z': 0.06614784896373749, 'pred_x': 0.025325659662485123, 'pred_z': 0.049512989819049835}, {'img_filename': 'sample_86.jpg', 'true_x': 0.16688090562820435, 'true_z': 0.04406430199742317, 'pred_x': 0.18268223106861115, 'pred_z': 0.038983918726444244}]\n",
      "Plotted ground truth and prediction on sample_2.jpg and saved as data/mocap_rb/model_validation/val/output_sample_2.jpg.\n",
      "Plotted ground truth and prediction on sample_9.jpg and saved as data/mocap_rb/model_validation/val/output_sample_9.jpg.\n",
      "Plotted ground truth and prediction on sample_21.jpg and saved as data/mocap_rb/model_validation/val/output_sample_21.jpg.\n",
      "Plotted ground truth and prediction on sample_24.jpg and saved as data/mocap_rb/model_validation/val/output_sample_24.jpg.\n",
      "Plotted ground truth and prediction on sample_38.jpg and saved as data/mocap_rb/model_validation/val/output_sample_38.jpg.\n",
      "Plotted ground truth and prediction on sample_43.jpg and saved as data/mocap_rb/model_validation/val/output_sample_43.jpg.\n",
      "Plotted ground truth and prediction on sample_51.jpg and saved as data/mocap_rb/model_validation/val/output_sample_51.jpg.\n",
      "Plotted ground truth and prediction on sample_60.jpg and saved as data/mocap_rb/model_validation/val/output_sample_60.jpg.\n",
      "Plotted ground truth and prediction on sample_79.jpg and saved as data/mocap_rb/model_validation/val/output_sample_79.jpg.\n",
      "Plotted ground truth and prediction on sample_86.jpg and saved as data/mocap_rb/model_validation/val/output_sample_86.jpg.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Placeholder for collecting results\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        inputs, labels, img_filename = val_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        #print(labels)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Collect the ground truth and predictions\n",
    "        results.append({\n",
    "            'img_filename': img_filename, \n",
    "            'true_x': labels[0, 6].item(), #x3\n",
    "            'true_z': labels[0, 8].item(), #z3\n",
    "            'pred_x': outputs[0, 6].item(), #x3\n",
    "            'pred_z': outputs[0, 8].item() #z3\n",
    "        })\n",
    "    print(results)\n",
    "\n",
    "data_dir = 'data/mocap_rb'\n",
    "output_folder = 'val'\n",
    "dataset_file = os.path.join(data_dir, 'single_img_regression_mocap_rb.csv')\n",
    "plot_predictions_on_images(results, data_dir, output_folder, dataset_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'img_filename': 'sample_0.jpg', 'true_x': 0.19473937153816223, 'true_z': 0.08740527927875519, 'pred_x': 0.1806333363056183, 'pred_z': 0.07974600046873093}, {'img_filename': 'sample_10.jpg', 'true_x': 0.12960763275623322, 'true_z': 0.09695424884557724, 'pred_x': 0.10531555116176605, 'pred_z': 0.09248902648687363}, {'img_filename': 'sample_12.jpg', 'true_x': 0.10266948491334915, 'true_z': 0.09998971223831177, 'pred_x': 0.11008936166763306, 'pred_z': 0.09914093464612961}, {'img_filename': 'sample_20.jpg', 'true_x': 0.10223014652729034, 'true_z': 0.11889924854040146, 'pred_x': 0.10184451937675476, 'pred_z': 0.10333433002233505}, {'img_filename': 'sample_32.jpg', 'true_x': -0.002320392755791545, 'true_z': 0.10276958346366882, 'pred_x': 0.023933250457048416, 'pred_z': 0.09703477472066879}, {'img_filename': 'sample_48.jpg', 'true_x': 0.15924419462680817, 'true_z': 0.03555695340037346, 'pred_x': 0.18974313139915466, 'pred_z': 0.04024738818407059}, {'img_filename': 'sample_53.jpg', 'true_x': 0.16265597939491272, 'true_z': 0.02928529679775238, 'pred_x': 0.18290665745735168, 'pred_z': 0.02375001274049282}, {'img_filename': 'sample_58.jpg', 'true_x': 0.18125393986701965, 'true_z': 0.11939456313848495, 'pred_x': 0.1868153065443039, 'pred_z': 0.13554392755031586}, {'img_filename': 'sample_70.jpg', 'true_x': 0.013041147962212563, 'true_z': 0.12719453871250153, 'pred_x': 0.02579750493168831, 'pred_z': 0.11634688824415207}]\n",
      "Plotted ground truth and prediction on sample_0.jpg and saved as data/mocap_rb/model_validation/test/output_sample_0.jpg.\n",
      "Plotted ground truth and prediction on sample_10.jpg and saved as data/mocap_rb/model_validation/test/output_sample_10.jpg.\n",
      "Plotted ground truth and prediction on sample_12.jpg and saved as data/mocap_rb/model_validation/test/output_sample_12.jpg.\n",
      "Plotted ground truth and prediction on sample_20.jpg and saved as data/mocap_rb/model_validation/test/output_sample_20.jpg.\n",
      "Plotted ground truth and prediction on sample_32.jpg and saved as data/mocap_rb/model_validation/test/output_sample_32.jpg.\n",
      "Plotted ground truth and prediction on sample_48.jpg and saved as data/mocap_rb/model_validation/test/output_sample_48.jpg.\n",
      "Plotted ground truth and prediction on sample_53.jpg and saved as data/mocap_rb/model_validation/test/output_sample_53.jpg.\n",
      "Plotted ground truth and prediction on sample_58.jpg and saved as data/mocap_rb/model_validation/test/output_sample_58.jpg.\n",
      "Plotted ground truth and prediction on sample_70.jpg and saved as data/mocap_rb/model_validation/test/output_sample_70.jpg.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Placeholder for collecting results\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs, labels, img_filename = test_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        #print(labels)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Collect the ground truth and predictions\n",
    "        results.append({\n",
    "            'img_filename': img_filename, \n",
    "            'true_x': labels[0, 6].item(),\n",
    "            'true_z': labels[0, 8].item(),\n",
    "            'pred_x': outputs[0, 6].item(),\n",
    "            'pred_z': outputs[0, 8].item()\n",
    "        })\n",
    "    print(results)\n",
    "\n",
    "\n",
    "data_dir = 'data/mocap_rb'\n",
    "output_folder = 'test'\n",
    "dataset_file = os.path.join(data_dir, 'single_img_regression_mocap_rb.csv')\n",
    "plot_predictions_on_images(results, data_dir, output_folder, dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time on CPU: 0.035125 seconds\n",
      "Average inference time on GPU: 0.002542 seconds\n",
      "gpu is 13 times faster than cpu\n",
      "cpu max inference rate = 28 Hz\n",
      "gpu max inference rate = 393 Hz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from math import floor\n",
    "\n",
    "# Assume `model` is your already trained model\n",
    "\n",
    "# Single image tensor (assuming it is already preprocessed)\n",
    "input_image = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 channels (RGB), 224x224 resolution\n",
    "\n",
    "# Ensure that the model is correctly loaded on the CPU\n",
    "model_cpu = model.to('cpu')\n",
    "model_cpu.eval()\n",
    "\n",
    "# Measure inference time on CPU\n",
    "input_image_cpu = input_image.to('cpu')  # Ensure the input tensor is on the CPU\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):  # Run inference 100 times to get an average\n",
    "        _ = model_cpu(input_image_cpu)\n",
    "    end_time = time.time()\n",
    "    avg_cpu_time = (end_time - start_time) / 100\n",
    "    print(f'Average inference time on CPU: {avg_cpu_time:.6f} seconds')\n",
    "\n",
    "# Measure inference time on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    model_gpu = model.to('cuda')\n",
    "    model_gpu.eval()\n",
    "    input_image_gpu = input_image.to('cuda')  # Ensure the input tensor is on the GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Warm-up GPU (GPU needs a warm-up time for the first operation)\n",
    "        _ = model_gpu(input_image_gpu)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):  # Run inference 100 times to get an average\n",
    "            _ = model_gpu(input_image_gpu)\n",
    "        # Wait for GPU to finish (synchronize)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        avg_gpu_time = (end_time - start_time) / 100\n",
    "        print(f'Average inference time on GPU: {avg_gpu_time:.6f} seconds')\n",
    "\n",
    "cpu_over_gpu = avg_cpu_time / avg_gpu_time\n",
    "print(f'gpu is {int(cpu_over_gpu)} times faster than cpu')\n",
    "\n",
    "cpu_hz = 1/avg_cpu_time\n",
    "gpu_hz = 1/avg_gpu_time\n",
    "print(f'cpu max inference rate = {floor(cpu_hz)} Hz')\n",
    "print(f'gpu max inference rate = {floor(gpu_hz)} Hz')\n",
    "\n",
    "# on ceres:\n",
    "# Average inference time on CPU: 0.035125 seconds\n",
    "# Average inference time on GPU: 0.002542 seconds\n",
    "# gpu is 13 times faster than cpu\n",
    "# cpu max inference rate = 28 Hz\n",
    "# gpu max inference rate = 393 Hz\n",
    "# but this varies from call to call\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
