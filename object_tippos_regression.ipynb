{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import csv\n",
    "import cv2\n",
    "import data_aug\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with trunk kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING DATA\n",
    "Trim dataset for single image set to one x3, y3, z3 set per image. This contains the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tip_pos_csv(input_csv, output_csv):\n",
    "    # Check if the output CSV exists and create it with the header if it doesn't\n",
    "    if not os.path.exists(output_csv):\n",
    "        header = ['ID', 'x3', 'y3', 'z3', 'img_filename']\n",
    "        with open(output_csv, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "\n",
    "    # Read the input CSV\n",
    "    positions_df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Initialize variables to keep track of the last image filename and the last row to write\n",
    "    last_img_filename = None\n",
    "    last_output_row = None\n",
    "\n",
    "    # Process each row in the input CSV\n",
    "    for index, row in positions_df.iterrows():\n",
    "        cur_img_filename = row['img_filename']\n",
    "        img_name = cur_img_filename.removesuffix(\".jpg\")\n",
    "        output_row = [row['ID'], row['x3'], row['y3'], row['z3'], cur_img_filename]\n",
    "        #output_row_augmented = [row['ID'], row['x3'], row['y3'], row['z3'], img_name+'_augmented.jpg']\n",
    "\n",
    "        # If the current image filename is different from the last, write the last output row\n",
    "        if last_img_filename is not None and last_img_filename != cur_img_filename:\n",
    "            with open(output_csv, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(last_output_row) #non-augmented image\n",
    "                #writer.writerow(last_output_row_augmented)# augmented image\n",
    "\n",
    "        # Update the last image filename and last output row\n",
    "        last_img_filename = cur_img_filename\n",
    "        last_output_row = output_row\n",
    "        #last_output_row_augmented = output_row_augmented\n",
    "\n",
    "    # Write the last row (for the final image in the sequence)\n",
    "    if last_output_row is not None:\n",
    "        with open(output_csv, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(last_output_row) # non-augmented image\n",
    "            #writer.writerow(last_output_row_augmented) # augmented image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/single'\n",
    "input_csv = os.path.join(data_dir, 'single_img_regression.csv')  # replace with your actual CSV filename\n",
    "output_csv = os.path.join(data_dir, 'single_img_regression_single_tip_pos.csv')\n",
    "make_tip_pos_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot tip position overlaid on images to verify dataset quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pillow_coords(df, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert robot coordinates to Pillow image coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'x' and 'z' columns in robot coordinates.\n",
    "    img_width, img_height (int): Dimensions of the Pillow image.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with 'img_x' and 'img_y' columns for Pillow image coordinates.\n",
    "    \"\"\"\n",
    "    buffer = .01 #m\n",
    "\n",
    "    # Calculate the min and max values for x and z\n",
    "    x_min, x_max = df['x'].min() - buffer, df['x'].max() + buffer\n",
    "    z_min, z_max = df['z'].min() - buffer, df['z'].max() + buffer\n",
    "\n",
    "    # Calculate the scaling factors for x and z coordinates\n",
    "    x_scale = img_width / (x_max - x_min)\n",
    "    z_scale = img_height / (z_max - z_min)\n",
    "\n",
    "    # Calculate the shifts to center the robot's origin within the image\n",
    "    x_shift = (x_max + x_min) / 2\n",
    "    z_shift = (z_max + z_min) / 2\n",
    "\n",
    "    # Convert robot coordinates to Pillow image coordinates\n",
    "    df['img_x'] = (df['x'] - x_shift) * x_scale + img_width / 2\n",
    "    df['img_y'] = (df['z'] - z_shift) * z_scale + img_height / 2\n",
    "\n",
    "    # Invert the y-axis and x-axis to match Pillow's coordinate system (where (0, 0) is top-left)\n",
    "    df['img_y'] = img_height - df['img_y']\n",
    "    df['img_x'] = img_width - df['img_x']\n",
    "\n",
    "    return df[['img_x', 'img_y']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tip_positions_on_images(data_dir):\n",
    "    \"\"\"\n",
    "    Plot tip positions on images based on coordinates from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    data_dir (str): Directory containing the CSV file and the images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file\n",
    "    csv_file = os.path.join(data_dir, 'single_img_regression_single_tip_pos.csv')\n",
    "    positions_df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Folder containing images\n",
    "    image_folder = os.path.join(data_dir, 'images')\n",
    "\n",
    "    # Convert entire DataFrame coordinates to Pillow image coordinates once\n",
    "    coords_df = positions_df[['x3', 'z3']].rename(columns={'x3': 'x', 'z3': 'z'})\n",
    "\n",
    "    # Get image dimensions from the first image\n",
    "    sample_image_filename = positions_df['img_filename'].iloc[0]\n",
    "    sample_image_path = os.path.join(image_folder, sample_image_filename)\n",
    "    with Image.open(sample_image_path) as img:\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "    # Convert all coordinates to image coordinates\n",
    "    img_coords_df = convert_to_pillow_coords(coords_df, img_width, img_height)\n",
    "\n",
    "    # Add converted coordinates to the original DataFrame\n",
    "    positions_df['img_x'] = img_coords_df['img_x']\n",
    "    positions_df['img_y'] = img_coords_df['img_y']\n",
    "\n",
    "    # Iterate through each row to plot points on the corresponding images\n",
    "    for index, row in positions_df.iterrows():\n",
    "        image_filename = row['img_filename']\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "\n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "\n",
    "            # Create a figure with the same dimensions as the image\n",
    "            fig, ax = plt.subplots(figsize=(img_width / 100, img_height / 100), dpi=100)\n",
    "\n",
    "            # Plot the image\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Plot the tip position for this specific row\n",
    "            ax.scatter([row['img_x']], [row['img_y']], color='blue', s=200)  # s is the size of the point\n",
    "\n",
    "            # Remove axes for a cleaner output\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Save the image with the original dimensions\n",
    "            output_filename = os.path.join(data_dir, f\"verification/output_{image_filename}\")\n",
    "            plt.savefig(output_filename, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Plotted tip position on {image_filename} and saved as {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted tip position on sample_0.jpg and saved as data/single/verification/output_sample_0.jpg.\n",
      "Plotted tip position on sample_1.jpg and saved as data/single/verification/output_sample_1.jpg.\n",
      "Plotted tip position on sample_2.jpg and saved as data/single/verification/output_sample_2.jpg.\n",
      "Plotted tip position on sample_3.jpg and saved as data/single/verification/output_sample_3.jpg.\n",
      "Plotted tip position on sample_4.jpg and saved as data/single/verification/output_sample_4.jpg.\n",
      "Plotted tip position on sample_5.jpg and saved as data/single/verification/output_sample_5.jpg.\n",
      "Plotted tip position on sample_6.jpg and saved as data/single/verification/output_sample_6.jpg.\n",
      "Plotted tip position on sample_7.jpg and saved as data/single/verification/output_sample_7.jpg.\n",
      "Plotted tip position on sample_8.jpg and saved as data/single/verification/output_sample_8.jpg.\n",
      "Plotted tip position on sample_9.jpg and saved as data/single/verification/output_sample_9.jpg.\n",
      "Plotted tip position on sample_10.jpg and saved as data/single/verification/output_sample_10.jpg.\n",
      "Plotted tip position on sample_11.jpg and saved as data/single/verification/output_sample_11.jpg.\n",
      "Plotted tip position on sample_12.jpg and saved as data/single/verification/output_sample_12.jpg.\n",
      "Plotted tip position on sample_13.jpg and saved as data/single/verification/output_sample_13.jpg.\n",
      "Plotted tip position on sample_14.jpg and saved as data/single/verification/output_sample_14.jpg.\n",
      "Plotted tip position on sample_15.jpg and saved as data/single/verification/output_sample_15.jpg.\n",
      "Plotted tip position on sample_16.jpg and saved as data/single/verification/output_sample_16.jpg.\n",
      "Plotted tip position on sample_17.jpg and saved as data/single/verification/output_sample_17.jpg.\n",
      "Plotted tip position on sample_18.jpg and saved as data/single/verification/output_sample_18.jpg.\n",
      "Plotted tip position on sample_19.jpg and saved as data/single/verification/output_sample_19.jpg.\n",
      "Plotted tip position on sample_20.jpg and saved as data/single/verification/output_sample_20.jpg.\n",
      "Plotted tip position on sample_21.jpg and saved as data/single/verification/output_sample_21.jpg.\n",
      "Plotted tip position on sample_22.jpg and saved as data/single/verification/output_sample_22.jpg.\n",
      "Plotted tip position on sample_23.jpg and saved as data/single/verification/output_sample_23.jpg.\n",
      "Plotted tip position on sample_24.jpg and saved as data/single/verification/output_sample_24.jpg.\n",
      "Plotted tip position on sample_25.jpg and saved as data/single/verification/output_sample_25.jpg.\n",
      "Plotted tip position on sample_26.jpg and saved as data/single/verification/output_sample_26.jpg.\n",
      "Plotted tip position on sample_27.jpg and saved as data/single/verification/output_sample_27.jpg.\n",
      "Plotted tip position on sample_28.jpg and saved as data/single/verification/output_sample_28.jpg.\n",
      "Plotted tip position on sample_29.jpg and saved as data/single/verification/output_sample_29.jpg.\n",
      "Plotted tip position on sample_30.jpg and saved as data/single/verification/output_sample_30.jpg.\n"
     ]
    }
   ],
   "source": [
    "plot_tip_positions_on_images('data/single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new csvs for train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/single'\n",
    "output_dir = 'data/single/split'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_file = os.path.join(input_dir, 'single_img_regression_single_tip_pos.csv')\n",
    "image_dir = os.path.join(input_dir, 'raw/images')\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Split the data (80-10-10)\n",
    "train_filenames, test_filenames = train_test_split(df['img_filename'].tolist(), test_size=0.1, random_state=42)\n",
    "train_filenames, val_filenames = train_test_split(train_filenames, test_size=0.12, random_state=42) # 0.12 * 0.9 = 0.1\n",
    "\n",
    "# Filter the original DataFrame to create train, validation, and test CSVs\n",
    "train_df = df[df['img_filename'].isin(train_filenames)]\n",
    "val_df = df[df['img_filename'].isin(val_filenames)]\n",
    "test_df = df[df['img_filename'].isin(test_filenames)]\n",
    "\n",
    "# Save the new CSVs\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "val_csv_path = os.path.join(output_dir, 'val.csv')\n",
    "test_csv_path = os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment images and update train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "test_dir = os.path.join(output_dir, 'images/test')\n",
    "val_dir = os.path.join(output_dir, 'images/val')\n",
    "train_dir = os.path.join(output_dir, \"images/train\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "importlib.reload(data_aug)\n",
    "\n",
    "# Augment the training images and update the CSV\n",
    "augmented_rows = []\n",
    "for filename in train_filenames:\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    augmented_filename = data_aug.augment_image(image_path, train_dir)\n",
    "    \n",
    "    # Add the new row for the augmented image to the augmented_rows list\n",
    "    new_row = train_df[train_df['img_filename'] == filename].copy()\n",
    "    new_row['img_filename'] = augmented_filename\n",
    "    augmented_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame for the augmented data and append to train_df\n",
    "augmented_df = pd.concat(augmented_rows)\n",
    "final_train_df = pd.concat([train_df, augmented_df])\n",
    "\n",
    "# Save the updated train CSV\n",
    "final_train_df.to_csv(train_csv_path, index=False)\n",
    "\n",
    "# Copy validation and test images and CSVs without augmentation\n",
    "for filename in val_filenames:\n",
    "    src_path = os.path.join(image_dir, filename)\n",
    "    # dst_path = os.path.join(val_dir, filename)\n",
    "    # shutil.copy(src_path, dst_path)\n",
    "    data_aug.crop_and_resize(src_path, val_dir)\n",
    "\n",
    "for filename in test_filenames:\n",
    "    src_path = os.path.join(image_dir, filename)\n",
    "    # dst_path = os.path.join(test_dir, filename)\n",
    "    # shutil.copy(src_path, dst_path)\n",
    "    data_aug.crop_and_resize(src_path, test_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/single/raw'\n",
    "output_dir = 'data/single/augmented'\n",
    "importlib.reload(data_aug)\n",
    "\n",
    "for filename in os.listdir(input_dir+\"/images\"):\n",
    "    if filename.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "        image_path = os.path.join(input_dir+\"/images/\", filename)\n",
    "        data_aug.augment_image(image_path, output_dir)\n",
    "\n",
    "# after cropping, the size of the images are (1080, 1671, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 4])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        labels = self.data_frame.iloc[idx, 1:4].values\n",
    "        labels = labels.astype('float').tolist()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(labels)\n",
    "\n",
    "# Dataset parameters\n",
    "csv_file = 'data/single/single_img_regression_single_tip_pos.csv'\n",
    "root_dir = 'data/single/augmented/images/'\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # image input size to model (224,224)\n",
    "    # out of memory error with 1080x1080, batch size 32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = ImageDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markleone/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/markleone/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the last fully connected layer to output 3 values (x3, y3, z3)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3) #output layers is 3-vector\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 286.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 163.12 MiB is free. Including non-PyTorch memory, this process has 11.53 GiB memory in use. Of the allocated memory 11.14 GiB is allocated by PyTorch, and 222.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/trunk/lib/python3.12/site-packages/torch/nn/modules/conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 286.00 MiB. GPU 0 has a total capacity of 11.90 GiB of which 163.12 MiB is free. Including non-PyTorch memory, this process has 11.53 GiB memory in use. Of the allocated memory 11.14 GiB is allocated by PyTorch, and 222.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "# for 224x224 took 5min on mac, took 2min on ceres cpu, took 30s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[ 0.07911618 -0.04242505  0.11051759]], Ground Truth: [-0.1637994  -0.22002804 -0.01108289]\n",
      "Predicted: [[ 0.10839201 -0.05397307  0.09346988]], Ground Truth: [-0.1637994  -0.22002804 -0.01108289]\n",
      "Predicted: [[ 0.4467522  -0.03234733  0.18067892]], Ground Truth: [ 0.1755904  -0.21333098  0.01334643]\n",
      "Predicted: [[ 0.41552413 -0.06970471  0.10188471]], Ground Truth: [ 0.1755904  -0.21333098  0.01334643]\n",
      "Predicted: [[ 0.24142988 -0.02351029 -0.03656781]], Ground Truth: [-0.00449705 -0.18612087 -0.16756463]\n"
     ]
    }
   ],
   "source": [
    "# Example prediction on a single image\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# testin on training set (should be able to memorize training data as a first step)\n",
    "with torch.no_grad():\n",
    "    for i in range(5):  # Test on the first 5 images\n",
    "        inputs, labels = dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        outputs = model(inputs)\n",
    "        print(f'Predicted: {outputs.cpu().numpy()}, Ground Truth: {labels.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: \n",
    "# save predicted tip positions, \n",
    "# save model outputs,\n",
    "# display predicted vs actual tip positions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
