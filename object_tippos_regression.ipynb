{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import csv\n",
    "import cv2\n",
    "import data_aug\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with trunk kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING DATA\n",
    "Trim dataset for single image set to one x3, y3, z3 set per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tip_pos_csv(input_csv, output_csv):\n",
    "    # Check if the output CSV exists and create it with the header if it doesn't\n",
    "    if not os.path.exists(output_csv):\n",
    "        header = ['ID', 'x3', 'y3', 'z3', 'img_filename']\n",
    "        with open(output_csv, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "\n",
    "    # Read the input CSV\n",
    "    positions_df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Initialize variables to keep track of the last image filename and the last row to write\n",
    "    last_img_filename = None\n",
    "    last_output_row = None\n",
    "\n",
    "    # Process each row in the input CSV\n",
    "    for index, row in positions_df.iterrows():\n",
    "        cur_img_filename = row['img_filename']\n",
    "        img_name = cur_img_filename.removesuffix(\".jpg\")\n",
    "        output_row = [row['ID'], row['x3'], row['y3'], row['z3'], cur_img_filename]\n",
    "        output_row_augmented = [row['ID'], row['x3'], row['y3'], row['z3'], img_name+'_augmented.jpg']\n",
    "\n",
    "        # If the current image filename is different from the last, write the last output row\n",
    "        if last_img_filename is not None and last_img_filename != cur_img_filename:\n",
    "            with open(output_csv, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(last_output_row) #non-augmented image\n",
    "                writer.writerow(last_output_row_augmented)# augmented image\n",
    "\n",
    "        # Update the last image filename and last output row\n",
    "        last_img_filename = cur_img_filename\n",
    "        last_output_row = output_row\n",
    "        last_output_row_augmented = output_row_augmented\n",
    "\n",
    "    # Write the last row (for the final image in the sequence)\n",
    "    if last_output_row is not None:\n",
    "        with open(output_csv, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(last_output_row) # non-augmented image\n",
    "            writer.writerow(last_output_row_augmented) # augmented image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/single'\n",
    "input_csv = os.path.join(data_dir, 'single_img_regression.csv')  # replace with your actual CSV filename\n",
    "output_csv = os.path.join(data_dir, 'single_img_regression_single_tip_pos.csv')\n",
    "make_tip_pos_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot tip position overlaid on images to verify dataset quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pillow_coords(df, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert robot coordinates to Pillow image coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'x' and 'z' columns in robot coordinates.\n",
    "    img_width, img_height (int): Dimensions of the Pillow image.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with 'img_x' and 'img_y' columns for Pillow image coordinates.\n",
    "    \"\"\"\n",
    "    buffer = .01 #m\n",
    "\n",
    "    # Calculate the min and max values for x and z\n",
    "    x_min, x_max = df['x'].min() - buffer, df['x'].max() + buffer\n",
    "    z_min, z_max = df['z'].min() - buffer, df['z'].max() + buffer\n",
    "\n",
    "    # Calculate the scaling factors for x and z coordinates\n",
    "    x_scale = img_width / (x_max - x_min)\n",
    "    z_scale = img_height / (z_max - z_min)\n",
    "\n",
    "    # Calculate the shifts to center the robot's origin within the image\n",
    "    x_shift = (x_max + x_min) / 2\n",
    "    z_shift = (z_max + z_min) / 2\n",
    "\n",
    "    # Convert robot coordinates to Pillow image coordinates\n",
    "    df['img_x'] = (df['x'] - x_shift) * x_scale + img_width / 2\n",
    "    df['img_y'] = (df['z'] - z_shift) * z_scale + img_height / 2\n",
    "\n",
    "    # Invert the y-axis and x-axis to match Pillow's coordinate system (where (0, 0) is top-left)\n",
    "    df['img_y'] = img_height - df['img_y']\n",
    "    df['img_x'] = img_width - df['img_x']\n",
    "\n",
    "    return df[['img_x', 'img_y']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def plot_tip_positions_on_images(data_dir):\n",
    "    \"\"\"\n",
    "    Plot tip positions on images based on coordinates from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    data_dir (str): Directory containing the CSV file and the images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CSV file\n",
    "    csv_file = os.path.join(data_dir, 'single_img_regression_single_tip_pos.csv')\n",
    "    positions_df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Folder containing images\n",
    "    image_folder = os.path.join(data_dir, 'images')\n",
    "\n",
    "    # Convert entire DataFrame coordinates to Pillow image coordinates once\n",
    "    coords_df = positions_df[['x3', 'z3']].rename(columns={'x3': 'x', 'z3': 'z'})\n",
    "\n",
    "    # Get image dimensions from the first image\n",
    "    sample_image_filename = positions_df['img_filename'].iloc[0]\n",
    "    sample_image_path = os.path.join(image_folder, sample_image_filename)\n",
    "    with Image.open(sample_image_path) as img:\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "    # Convert all coordinates to image coordinates\n",
    "    img_coords_df = convert_to_pillow_coords(coords_df, img_width, img_height)\n",
    "\n",
    "    # Add converted coordinates to the original DataFrame\n",
    "    positions_df['img_x'] = img_coords_df['img_x']\n",
    "    positions_df['img_y'] = img_coords_df['img_y']\n",
    "\n",
    "    # Iterate through each row to plot points on the corresponding images\n",
    "    for index, row in positions_df.iterrows():\n",
    "        image_filename = row['img_filename']\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "\n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "\n",
    "            # Create a figure with the same dimensions as the image\n",
    "            fig, ax = plt.subplots(figsize=(img_width / 100, img_height / 100), dpi=100)\n",
    "\n",
    "            # Plot the image\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Plot the tip position for this specific row\n",
    "            ax.scatter([row['img_x']], [row['img_y']], color='blue', s=200)  # s is the size of the point\n",
    "\n",
    "            # Remove axes for a cleaner output\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Save the image with the original dimensions\n",
    "            output_filename = os.path.join(data_dir, f\"verification/output_{image_filename}\")\n",
    "            plt.savefig(output_filename, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Plotted tip position on {image_filename} and saved as {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotted tip position on sample_0.jpg and saved as data/single/verification/output_sample_0.jpg.\n",
      "Plotted tip position on sample_1.jpg and saved as data/single/verification/output_sample_1.jpg.\n",
      "Plotted tip position on sample_2.jpg and saved as data/single/verification/output_sample_2.jpg.\n",
      "Plotted tip position on sample_3.jpg and saved as data/single/verification/output_sample_3.jpg.\n",
      "Plotted tip position on sample_4.jpg and saved as data/single/verification/output_sample_4.jpg.\n",
      "Plotted tip position on sample_5.jpg and saved as data/single/verification/output_sample_5.jpg.\n",
      "Plotted tip position on sample_6.jpg and saved as data/single/verification/output_sample_6.jpg.\n",
      "Plotted tip position on sample_7.jpg and saved as data/single/verification/output_sample_7.jpg.\n",
      "Plotted tip position on sample_8.jpg and saved as data/single/verification/output_sample_8.jpg.\n",
      "Plotted tip position on sample_9.jpg and saved as data/single/verification/output_sample_9.jpg.\n",
      "Plotted tip position on sample_10.jpg and saved as data/single/verification/output_sample_10.jpg.\n",
      "Plotted tip position on sample_11.jpg and saved as data/single/verification/output_sample_11.jpg.\n",
      "Plotted tip position on sample_12.jpg and saved as data/single/verification/output_sample_12.jpg.\n",
      "Plotted tip position on sample_13.jpg and saved as data/single/verification/output_sample_13.jpg.\n",
      "Plotted tip position on sample_14.jpg and saved as data/single/verification/output_sample_14.jpg.\n",
      "Plotted tip position on sample_15.jpg and saved as data/single/verification/output_sample_15.jpg.\n",
      "Plotted tip position on sample_16.jpg and saved as data/single/verification/output_sample_16.jpg.\n",
      "Plotted tip position on sample_17.jpg and saved as data/single/verification/output_sample_17.jpg.\n",
      "Plotted tip position on sample_18.jpg and saved as data/single/verification/output_sample_18.jpg.\n",
      "Plotted tip position on sample_19.jpg and saved as data/single/verification/output_sample_19.jpg.\n",
      "Plotted tip position on sample_20.jpg and saved as data/single/verification/output_sample_20.jpg.\n",
      "Plotted tip position on sample_21.jpg and saved as data/single/verification/output_sample_21.jpg.\n",
      "Plotted tip position on sample_22.jpg and saved as data/single/verification/output_sample_22.jpg.\n",
      "Plotted tip position on sample_23.jpg and saved as data/single/verification/output_sample_23.jpg.\n",
      "Plotted tip position on sample_24.jpg and saved as data/single/verification/output_sample_24.jpg.\n",
      "Plotted tip position on sample_25.jpg and saved as data/single/verification/output_sample_25.jpg.\n",
      "Plotted tip position on sample_26.jpg and saved as data/single/verification/output_sample_26.jpg.\n",
      "Plotted tip position on sample_27.jpg and saved as data/single/verification/output_sample_27.jpg.\n",
      "Plotted tip position on sample_28.jpg and saved as data/single/verification/output_sample_28.jpg.\n",
      "Plotted tip position on sample_29.jpg and saved as data/single/verification/output_sample_29.jpg.\n",
      "Plotted tip position on sample_30.jpg and saved as data/single/verification/output_sample_30.jpg.\n"
     ]
    }
   ],
   "source": [
    "plot_tip_positions_on_images('data/single')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/single/raw'\n",
    "output_dir = 'data/single/augmented'\n",
    "importlib.reload(data_aug)\n",
    "\n",
    "for filename in os.listdir(input_dir+\"/images\"):\n",
    "    if filename.lower().endswith(('png', 'jpg', 'jpeg')):\n",
    "        image_path = os.path.join(input_dir+\"/images/\", filename)\n",
    "        data_aug.augment_image(image_path, output_dir)\n",
    "\n",
    "# after cropping, the size of the images are (1080, 1671, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 4])\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        labels = self.data_frame.iloc[idx, 1:4].values\n",
    "        labels = labels.astype('float').tolist()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(labels)\n",
    "\n",
    "# Dataset parameters\n",
    "csv_file = 'data/single/single_img_regression_single_tip_pos.csv'\n",
    "root_dir = 'data/single/augmented/images/'\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # image input size to model\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = ImageDataset(csv_file=csv_file, root_dir=root_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/asltrunk/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/asltrunk/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/asltrunk/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the last fully connected layer to output 3 values (x3, y3, z3)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3) #output layers is 3-vector\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.4195\n",
      "Epoch [2/20], Loss: 0.4380\n",
      "Epoch [3/20], Loss: 0.0103\n",
      "Epoch [4/20], Loss: 0.0100\n",
      "Epoch [5/20], Loss: 0.0080\n",
      "Epoch [6/20], Loss: 0.0052\n",
      "Epoch [7/20], Loss: 0.0052\n",
      "Epoch [8/20], Loss: 0.0038\n",
      "Epoch [9/20], Loss: 0.0039\n",
      "Epoch [10/20], Loss: 0.0029\n",
      "Epoch [11/20], Loss: 0.0025\n",
      "Epoch [12/20], Loss: 0.0022\n",
      "Epoch [13/20], Loss: 0.0018\n",
      "Epoch [14/20], Loss: 0.0017\n",
      "Epoch [15/20], Loss: 0.0013\n",
      "Epoch [16/20], Loss: 0.0013\n",
      "Epoch [17/20], Loss: 0.0012\n",
      "Epoch [18/20], Loss: 0.0017\n",
      "Epoch [19/20], Loss: 0.0009\n",
      "Epoch [20/20], Loss: 0.0023\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "#took 5min on mac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[-0.60934645  0.10587105  0.18116018]], Ground Truth: [-0.1637994  -0.22002804 -0.01108289]\n",
      "Predicted: [[-0.57156754  0.11285207  0.18355541]], Ground Truth: [-0.1637994  -0.22002804 -0.01108289]\n",
      "Predicted: [[-0.24884418  0.09402524  0.22770816]], Ground Truth: [ 0.1755904  -0.21333098  0.01334643]\n",
      "Predicted: [[-0.30379704  0.06469792  0.192056  ]], Ground Truth: [ 0.1755904  -0.21333098  0.01334643]\n",
      "Predicted: [[-0.45932662  0.10767904  0.00098242]], Ground Truth: [-0.00449705 -0.18612087 -0.16756463]\n"
     ]
    }
   ],
   "source": [
    "# Example prediction on a single image\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# testin on training set (should be able to memorize training data as a first step)\n",
    "with torch.no_grad():\n",
    "    for i in range(5):  # Test on the first 5 images\n",
    "        inputs, labels = dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        outputs = model(inputs)\n",
    "        print(f'Predicted: {outputs.cpu().numpy()}, Ground Truth: {labels.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
