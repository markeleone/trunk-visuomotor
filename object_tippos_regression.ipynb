{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image \n",
    "import csv\n",
    "import cv2\n",
    "import data_aug\n",
    "import importlib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with trunk kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESSING DATA\n",
    "Trim dataset for single image set to one x3, y3, z3 set per image. This contains the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_rb_pos_csv(input_csv, output_csv):\n",
    "    # Check if the output CSV exists and create it with the header if it doesn't\n",
    "    if not os.path.exists(output_csv):\n",
    "        header = ['ID', 'x1', 'y1', 'z1', 'x2', 'y2', 'z2', 'x3', 'y3', 'z3', 'isGripperOpen', 'img_filename']\n",
    "        with open(output_csv, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(header)\n",
    "\n",
    "    # Read the input CSV\n",
    "    positions_df = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Initialize variables to keep track of the last image filename and the last row to write\n",
    "    last_img_filename = None\n",
    "    last_output_row = None\n",
    "\n",
    "    # Process each row in the input CSV\n",
    "    for index, row in positions_df.iterrows():\n",
    "        cur_img_filename = row['img_filename']\n",
    "        #output_row = [row['ID'], row['x3'], row['y3'], row['z3'], cur_img_filename] #for tip position\n",
    "        output_row = row\n",
    "\n",
    "        # If the current image filename is different from the last, write the last output row\n",
    "        if last_img_filename is not None and last_img_filename != cur_img_filename:\n",
    "            with open(output_csv, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(last_output_row) \n",
    "\n",
    "        # Update the last image filename and last output row\n",
    "        last_img_filename = cur_img_filename\n",
    "        last_output_row = output_row\n",
    "        #last_output_row_augmented = output_row_augmented\n",
    "\n",
    "    # Write the last row (for the final image in the sequence)\n",
    "    if last_output_row is not None:\n",
    "        with open(output_csv, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(last_output_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/rigidbodies'\n",
    "input_csv = os.path.join(data_dir, 'raw/single_img_regression.csv')  # replace with your actual CSV filename\n",
    "output_csv = os.path.join(data_dir, 'single_img_regression_single_rb_pos.csv')\n",
    "make_rb_pos_csv(input_csv, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot tip position overlaid on images to verify dataset quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pillow_coords(df, img_width, img_height, x_min, x_max, z_min, z_max):\n",
    "    \"\"\"\n",
    "    Convert robot coordinates to Pillow image coordinates.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing 'x' and 'z' columns in robot coordinates.\n",
    "    img_width, img_height (int): Dimensions of the Pillow image.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with 'img_x' and 'img_y' columns for Pillow image coordinates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the scaling factors for x and z coordinates\n",
    "    x_scale = img_width / (x_max - x_min)\n",
    "    z_scale = img_height / (z_max - z_min)\n",
    "\n",
    "    # Calculate the shifts to center the robot's origin within the image\n",
    "    x_shift = (x_max + x_min) / 2\n",
    "    z_shift = (z_max + z_min) / 2\n",
    "\n",
    "    # Convert robot coordinates to Pillow image coordinates\n",
    "    df['img_x'] = (df['x'] - x_shift) * x_scale + img_width / 2\n",
    "    df['img_y'] = (df['z'] - z_shift) * z_scale + img_height / 2\n",
    "\n",
    "    # Invert the y-axis and x-axis to match Pillow's coordinate system (where (0, 0) is top-left)\n",
    "    df['img_y'] = img_height - df['img_y']\n",
    "    df['img_x'] = img_width - df['img_x']\n",
    "\n",
    "    return df[['img_x', 'img_y']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_on_images(results, data_dir, output_folder, dataset_file):\n",
    "    \"\"\"\n",
    "    Plot ground truth and predicted tip positions on images.\n",
    "\n",
    "    Parameters:\n",
    "    results (list): List of dictionaries containing img_filename, true_x, true_z, pred_x, pred_z\n",
    "    data_dir (str): Directory containing the images.\n",
    "    \"\"\"\n",
    "    image_folder = os.path.join(data_dir, 'raw/images')\n",
    "\n",
    "    for result in results:\n",
    "        image_filename = result['img_filename']\n",
    "        image_path = os.path.join(image_folder, image_filename)\n",
    "\n",
    "        # Open the image\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "\n",
    "            # Convert ground truth and predicted coordinates to image coordinates\n",
    "            positions_df = pd.read_csv(dataset_file)\n",
    "\n",
    "\n",
    "            # Convert entire DataFrame coordinates to Pillow image coordinates once\n",
    "            overall_df = positions_df[['x3', 'z3']].rename(columns={'x3': 'x', 'z3': 'z'})\n",
    "\n",
    "            #calculate extent of dataset for scaling to pillow coords\n",
    "            x_min, x_max = overall_df['x'].min() - 0.01, overall_df['x'].max() + 0.01\n",
    "            z_min, z_max = overall_df['z'].min() - 0.01, overall_df['z'].max() + 0.01\n",
    "\n",
    "            true_img_coords = convert_to_pillow_coords(pd.DataFrame({'x': [result['true_x']], 'z': [result['true_z']]}), img_width, img_height, x_min, x_max, z_min, z_max)\n",
    "            pred_img_coords = convert_to_pillow_coords(pd.DataFrame({'x': [result['pred_x']], 'z': [result['pred_z']]}), img_width, img_height,  x_min, x_max, z_min, z_max)\n",
    "\n",
    "            # Create a figure with the same dimensions as the image\n",
    "            fig, ax = plt.subplots(figsize=(img_width / 100, img_height / 100), dpi=100)\n",
    "\n",
    "            # Plot the image\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Plot the ground truth tip position\n",
    "            ax.scatter([true_img_coords['img_x'][0]], [true_img_coords['img_y'][0]], color='green', s=200, label='Ground Truth')\n",
    "\n",
    "            # Plot the predicted tip position\n",
    "            ax.scatter([pred_img_coords['img_x'][0]], [pred_img_coords['img_y'][0]], color='red', s=200, label='Prediction')\n",
    "\n",
    "            # Remove axes for a cleaner output\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add a legend\n",
    "            ax.legend()\n",
    "\n",
    "            # Save the image with the original dimensions\n",
    "            output_filename = os.path.join(data_dir, f\"model_validation/{output_folder}/output_{image_filename}\")\n",
    "            plt.savefig(output_filename, bbox_inches='tight', pad_inches=0)\n",
    "            plt.close(fig)\n",
    "\n",
    "            print(f\"Plotted ground truth and prediction on {image_filename} and saved as {output_filename}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-val-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new csvs for train, val, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'data/rigidbodies'\n",
    "output_dir = 'data/rigidbodies/split'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "csv_file = os.path.join(input_dir, 'single_img_regression_single_rb_pos.csv')\n",
    "image_dir = os.path.join(input_dir, 'raw/images')\n",
    "\n",
    "# Load the original CSV file\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Split the data (80-10-10)\n",
    "train_filenames, test_filenames = train_test_split(df['img_filename'].tolist(), test_size=0.1, random_state=42)\n",
    "train_filenames, val_filenames = train_test_split(train_filenames, test_size=0.12, random_state=42) # 0.12 * 0.9 = 0.1\n",
    "\n",
    "# Filter the original DataFrame to create train, validation, and test CSVs\n",
    "train_df = df[df['img_filename'].isin(train_filenames)]\n",
    "val_df = df[df['img_filename'].isin(val_filenames)]\n",
    "test_df = df[df['img_filename'].isin(test_filenames)]\n",
    "\n",
    "# Save the new CSVs\n",
    "train_csv_path = os.path.join(output_dir, 'train.csv')\n",
    "val_csv_path = os.path.join(output_dir, 'val.csv')\n",
    "test_csv_path = os.path.join(output_dir, 'test.csv')\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment images and update train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist\n",
    "test_dir = os.path.join(output_dir, 'images/test')\n",
    "val_dir = os.path.join(output_dir, 'images/val')\n",
    "train_dir = os.path.join(output_dir, \"images/train\")\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "importlib.reload(data_aug)\n",
    "\n",
    "# Augment the training images and update the CSV\n",
    "augmented_rows = []\n",
    "for filename in train_filenames:\n",
    "    image_path = os.path.join(image_dir, filename)\n",
    "    augmented_filename = data_aug.augment_image(image_path, train_dir)\n",
    "    \n",
    "    # Add the new row for the augmented image to the augmented_rows list\n",
    "    new_row = train_df[train_df['img_filename'] == filename].copy()\n",
    "    new_row['img_filename'] = augmented_filename\n",
    "    augmented_rows.append(new_row)\n",
    "\n",
    "# Create a new DataFrame for the augmented data and append to train_df\n",
    "augmented_df = pd.concat(augmented_rows)\n",
    "final_train_df = pd.concat([train_df, augmented_df])\n",
    "\n",
    "# Save the updated train CSV\n",
    "final_train_df.to_csv(train_csv_path, index=False)\n",
    "\n",
    "# Copy validation and test images and CSVs without augmentation\n",
    "for filename in val_filenames:\n",
    "    src_path = os.path.join(image_dir, filename)\n",
    "    data_aug.crop_and_resize(src_path, val_dir)\n",
    "\n",
    "for filename in test_filenames:\n",
    "    src_path = os.path.join(image_dir, filename)\n",
    "    data_aug.crop_and_resize(src_path, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx]['img_filename'])  # Use column name\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        labels = self.data_frame.iloc[idx, 1:10].values\n",
    "        labels = labels.astype('float').tolist()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Return image, labels, and the image filename\n",
    "        return image, torch.tensor(labels), self.data_frame.iloc[idx]['img_filename']  # Use column name\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # image input size to model (224,224)\n",
    "    # out of memory error with 1080x1080, batch size 32\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "# Function to calculate RMSE\n",
    "criterion = nn.MSELoss()\n",
    "def calculate_rmse(outputs, labels):\n",
    "    mse_loss = criterion(outputs, labels)\n",
    "    return torch.sqrt(mse_loss).item()\n",
    "\n",
    "# Dataset parameters\n",
    "data_dir = 'data/rigidbodies/split' #just change this when you change datasets\n",
    "\n",
    "train_csv_file = os.path.join(data_dir, 'train.csv')\n",
    "train_img_dir = os.path.join(data_dir, 'images/train')\n",
    "\n",
    "val_csv_file = os.path.join(data_dir, 'val.csv')\n",
    "val_img_dir = os.path.join(data_dir, 'images/val')\n",
    "\n",
    "test_csv_file = os.path.join(data_dir, 'test.csv')\n",
    "test_img_dir = os.path.join(data_dir, 'images/test')\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageDataset(csv_file=train_csv_file, root_dir=train_img_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = ImageDataset(csv_file=val_csv_file, root_dir=val_img_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = ImageDataset(csv_file=test_csv_file, root_dir=test_img_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check CUDA available (should be on ceres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markleone/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/markleone/miniconda3/envs/trunk/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained ResNet model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the last fully connected layer to output 9 values (x1, y1, z1, x2, y2, z2, x3, y3, z3)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 9) #output layers is 9-vector\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 0.3826, Train RMSE: 0.6036, Val RMSE: 0.7679\n",
      "Epoch [2/200], Loss: 0.0890, Train RMSE: 0.2795, Val RMSE: 0.9371\n",
      "Epoch [3/200], Loss: 0.0128, Train RMSE: 0.1129, Val RMSE: 1.1930\n",
      "Epoch [4/200], Loss: 0.0047, Train RMSE: 0.0687, Val RMSE: 1.4599\n",
      "Epoch [5/200], Loss: 0.0037, Train RMSE: 0.0605, Val RMSE: 1.4165\n",
      "Epoch [6/200], Loss: 0.0026, Train RMSE: 0.0505, Val RMSE: 1.2237\n",
      "Epoch [7/200], Loss: 0.0022, Train RMSE: 0.0471, Val RMSE: 1.0691\n",
      "Epoch [8/200], Loss: 0.0015, Train RMSE: 0.0387, Val RMSE: 0.9501\n",
      "Epoch [9/200], Loss: 0.0016, Train RMSE: 0.0396, Val RMSE: 0.8161\n",
      "Epoch [10/200], Loss: 0.0012, Train RMSE: 0.0348, Val RMSE: 0.7119\n",
      "Epoch [11/200], Loss: 0.0010, Train RMSE: 0.0313, Val RMSE: 0.6212\n",
      "Epoch [12/200], Loss: 0.0006, Train RMSE: 0.0255, Val RMSE: 0.5116\n",
      "Epoch [13/200], Loss: 0.0008, Train RMSE: 0.0289, Val RMSE: 0.4142\n",
      "Epoch [14/200], Loss: 0.0006, Train RMSE: 0.0247, Val RMSE: 0.3569\n",
      "Epoch [15/200], Loss: 0.0005, Train RMSE: 0.0227, Val RMSE: 0.3260\n",
      "Epoch [16/200], Loss: 0.0004, Train RMSE: 0.0192, Val RMSE: 0.2784\n",
      "Epoch [17/200], Loss: 0.0006, Train RMSE: 0.0236, Val RMSE: 0.2180\n",
      "Epoch [18/200], Loss: 0.0007, Train RMSE: 0.0250, Val RMSE: 0.1855\n",
      "Epoch [19/200], Loss: 0.0004, Train RMSE: 0.0193, Val RMSE: 0.1649\n",
      "Epoch [20/200], Loss: 0.0007, Train RMSE: 0.0268, Val RMSE: 0.1370\n",
      "Epoch [21/200], Loss: 0.0007, Train RMSE: 0.0268, Val RMSE: 0.1180\n",
      "Epoch [22/200], Loss: 0.0008, Train RMSE: 0.0266, Val RMSE: 0.0997\n",
      "Epoch [23/200], Loss: 0.0003, Train RMSE: 0.0171, Val RMSE: 0.0820\n",
      "Epoch [24/200], Loss: 0.0006, Train RMSE: 0.0237, Val RMSE: 0.0694\n",
      "Epoch [25/200], Loss: 0.0005, Train RMSE: 0.0207, Val RMSE: 0.0610\n",
      "Epoch [26/200], Loss: 0.0009, Train RMSE: 0.0294, Val RMSE: 0.0499\n",
      "Epoch [27/200], Loss: 0.0007, Train RMSE: 0.0236, Val RMSE: 0.0488\n",
      "Epoch [28/200], Loss: 0.0008, Train RMSE: 0.0275, Val RMSE: 0.0422\n",
      "Epoch [29/200], Loss: 0.0002, Train RMSE: 0.0150, Val RMSE: 0.0387\n",
      "Epoch [30/200], Loss: 0.0005, Train RMSE: 0.0217, Val RMSE: 0.0311\n",
      "Epoch [31/200], Loss: 0.0008, Train RMSE: 0.0242, Val RMSE: 0.0327\n",
      "Epoch [32/200], Loss: 0.0004, Train RMSE: 0.0202, Val RMSE: 0.0261\n",
      "Epoch [33/200], Loss: 0.0002, Train RMSE: 0.0144, Val RMSE: 0.0293\n",
      "Epoch [34/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0255\n",
      "Epoch [35/200], Loss: 0.0003, Train RMSE: 0.0173, Val RMSE: 0.0253\n",
      "Epoch [36/200], Loss: 0.0004, Train RMSE: 0.0188, Val RMSE: 0.0222\n",
      "Epoch [37/200], Loss: 0.0011, Train RMSE: 0.0293, Val RMSE: 0.0224\n",
      "Epoch [38/200], Loss: 0.0004, Train RMSE: 0.0176, Val RMSE: 0.0278\n",
      "Epoch [39/200], Loss: 0.0005, Train RMSE: 0.0209, Val RMSE: 0.0301\n",
      "Epoch [40/200], Loss: 0.0013, Train RMSE: 0.0310, Val RMSE: 0.0224\n",
      "Epoch [41/200], Loss: 0.0012, Train RMSE: 0.0298, Val RMSE: 0.0251\n",
      "Epoch [42/200], Loss: 0.0003, Train RMSE: 0.0186, Val RMSE: 0.0371\n",
      "Epoch [43/200], Loss: 0.0010, Train RMSE: 0.0313, Val RMSE: 0.0230\n",
      "Epoch [44/200], Loss: 0.0005, Train RMSE: 0.0217, Val RMSE: 0.0296\n",
      "Epoch [45/200], Loss: 0.0006, Train RMSE: 0.0238, Val RMSE: 0.0266\n",
      "Epoch [46/200], Loss: 0.0003, Train RMSE: 0.0178, Val RMSE: 0.0256\n",
      "Epoch [47/200], Loss: 0.0002, Train RMSE: 0.0153, Val RMSE: 0.0245\n",
      "Epoch [48/200], Loss: 0.0002, Train RMSE: 0.0152, Val RMSE: 0.0209\n",
      "Epoch [49/200], Loss: 0.0002, Train RMSE: 0.0120, Val RMSE: 0.0207\n",
      "Epoch [50/200], Loss: 0.0001, Train RMSE: 0.0104, Val RMSE: 0.0242\n",
      "Epoch [51/200], Loss: 0.0002, Train RMSE: 0.0124, Val RMSE: 0.0207\n",
      "Epoch [52/200], Loss: 0.0004, Train RMSE: 0.0186, Val RMSE: 0.0214\n",
      "Epoch [53/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0228\n",
      "Epoch [54/200], Loss: 0.0005, Train RMSE: 0.0208, Val RMSE: 0.0235\n",
      "Epoch [55/200], Loss: 0.0003, Train RMSE: 0.0186, Val RMSE: 0.0220\n",
      "Epoch [56/200], Loss: 0.0001, Train RMSE: 0.0111, Val RMSE: 0.0242\n",
      "Epoch [57/200], Loss: 0.0004, Train RMSE: 0.0180, Val RMSE: 0.0222\n",
      "Epoch [58/200], Loss: 0.0003, Train RMSE: 0.0180, Val RMSE: 0.0241\n",
      "Epoch [59/200], Loss: 0.0002, Train RMSE: 0.0128, Val RMSE: 0.0257\n",
      "Epoch [60/200], Loss: 0.0003, Train RMSE: 0.0175, Val RMSE: 0.0217\n",
      "Epoch [61/200], Loss: 0.0003, Train RMSE: 0.0163, Val RMSE: 0.0225\n",
      "Epoch [62/200], Loss: 0.0005, Train RMSE: 0.0196, Val RMSE: 0.0296\n",
      "Epoch [63/200], Loss: 0.0005, Train RMSE: 0.0223, Val RMSE: 0.0210\n",
      "Epoch [64/200], Loss: 0.0003, Train RMSE: 0.0160, Val RMSE: 0.0277\n",
      "Epoch [65/200], Loss: 0.0005, Train RMSE: 0.0222, Val RMSE: 0.0264\n",
      "Epoch [66/200], Loss: 0.0004, Train RMSE: 0.0209, Val RMSE: 0.0238\n",
      "Epoch [67/200], Loss: 0.0002, Train RMSE: 0.0154, Val RMSE: 0.0276\n",
      "Epoch [68/200], Loss: 0.0004, Train RMSE: 0.0165, Val RMSE: 0.0236\n",
      "Epoch [69/200], Loss: 0.0003, Train RMSE: 0.0156, Val RMSE: 0.0207\n",
      "Epoch [70/200], Loss: 0.0001, Train RMSE: 0.0118, Val RMSE: 0.0205\n",
      "Epoch [71/200], Loss: 0.0001, Train RMSE: 0.0112, Val RMSE: 0.0194\n",
      "Epoch [72/200], Loss: 0.0002, Train RMSE: 0.0139, Val RMSE: 0.0194\n",
      "Epoch [73/200], Loss: 0.0001, Train RMSE: 0.0096, Val RMSE: 0.0223\n",
      "Epoch [74/200], Loss: 0.0002, Train RMSE: 0.0153, Val RMSE: 0.0222\n",
      "Epoch [75/200], Loss: 0.0002, Train RMSE: 0.0142, Val RMSE: 0.0233\n",
      "Epoch [76/200], Loss: 0.0008, Train RMSE: 0.0276, Val RMSE: 0.0233\n",
      "Epoch [77/200], Loss: 0.0004, Train RMSE: 0.0187, Val RMSE: 0.0225\n",
      "Epoch [78/200], Loss: 0.0009, Train RMSE: 0.0289, Val RMSE: 0.0204\n",
      "Epoch [79/200], Loss: 0.0001, Train RMSE: 0.0089, Val RMSE: 0.0221\n",
      "Epoch [80/200], Loss: 0.0003, Train RMSE: 0.0176, Val RMSE: 0.0247\n",
      "Epoch [81/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0234\n",
      "Epoch [82/200], Loss: 0.0006, Train RMSE: 0.0217, Val RMSE: 0.0211\n",
      "Epoch [83/200], Loss: 0.0005, Train RMSE: 0.0228, Val RMSE: 0.0252\n",
      "Epoch [84/200], Loss: 0.0004, Train RMSE: 0.0184, Val RMSE: 0.0252\n",
      "Epoch [85/200], Loss: 0.0004, Train RMSE: 0.0187, Val RMSE: 0.0229\n",
      "Epoch [86/200], Loss: 0.0006, Train RMSE: 0.0248, Val RMSE: 0.0332\n",
      "Epoch [87/200], Loss: 0.0006, Train RMSE: 0.0232, Val RMSE: 0.0231\n",
      "Epoch [88/200], Loss: 0.0005, Train RMSE: 0.0226, Val RMSE: 0.0233\n",
      "Epoch [89/200], Loss: 0.0003, Train RMSE: 0.0173, Val RMSE: 0.0211\n",
      "Epoch [90/200], Loss: 0.0003, Train RMSE: 0.0182, Val RMSE: 0.0219\n",
      "Epoch [91/200], Loss: 0.0003, Train RMSE: 0.0156, Val RMSE: 0.0218\n",
      "Epoch [92/200], Loss: 0.0002, Train RMSE: 0.0150, Val RMSE: 0.0206\n",
      "Epoch [93/200], Loss: 0.0002, Train RMSE: 0.0134, Val RMSE: 0.0325\n",
      "Epoch [94/200], Loss: 0.0008, Train RMSE: 0.0275, Val RMSE: 0.0266\n",
      "Epoch [95/200], Loss: 0.0005, Train RMSE: 0.0213, Val RMSE: 0.0263\n",
      "Epoch [96/200], Loss: 0.0006, Train RMSE: 0.0240, Val RMSE: 0.0363\n",
      "Epoch [97/200], Loss: 0.0008, Train RMSE: 0.0278, Val RMSE: 0.0205\n",
      "Epoch [98/200], Loss: 0.0007, Train RMSE: 0.0238, Val RMSE: 0.0264\n",
      "Epoch [99/200], Loss: 0.0003, Train RMSE: 0.0175, Val RMSE: 0.0292\n",
      "Epoch [100/200], Loss: 0.0004, Train RMSE: 0.0201, Val RMSE: 0.0243\n",
      "Epoch [101/200], Loss: 0.0011, Train RMSE: 0.0291, Val RMSE: 0.0324\n",
      "Epoch [102/200], Loss: 0.0008, Train RMSE: 0.0278, Val RMSE: 0.0253\n",
      "Epoch [103/200], Loss: 0.0006, Train RMSE: 0.0246, Val RMSE: 0.0265\n",
      "Epoch [104/200], Loss: 0.0005, Train RMSE: 0.0211, Val RMSE: 0.0212\n",
      "Epoch [105/200], Loss: 0.0004, Train RMSE: 0.0190, Val RMSE: 0.0244\n",
      "Epoch [106/200], Loss: 0.0008, Train RMSE: 0.0258, Val RMSE: 0.0210\n",
      "Epoch [107/200], Loss: 0.0005, Train RMSE: 0.0205, Val RMSE: 0.0265\n",
      "Epoch [108/200], Loss: 0.0003, Train RMSE: 0.0175, Val RMSE: 0.0363\n",
      "Epoch [109/200], Loss: 0.0012, Train RMSE: 0.0317, Val RMSE: 0.0389\n",
      "Epoch [110/200], Loss: 0.0010, Train RMSE: 0.0317, Val RMSE: 0.0211\n",
      "Epoch [111/200], Loss: 0.0003, Train RMSE: 0.0167, Val RMSE: 0.0301\n",
      "Epoch [112/200], Loss: 0.0009, Train RMSE: 0.0301, Val RMSE: 0.0304\n",
      "Epoch [113/200], Loss: 0.0004, Train RMSE: 0.0180, Val RMSE: 0.0225\n",
      "Epoch [114/200], Loss: 0.0003, Train RMSE: 0.0164, Val RMSE: 0.0259\n",
      "Epoch [115/200], Loss: 0.0007, Train RMSE: 0.0260, Val RMSE: 0.0295\n",
      "Epoch [116/200], Loss: 0.0006, Train RMSE: 0.0241, Val RMSE: 0.0257\n",
      "Epoch [117/200], Loss: 0.0002, Train RMSE: 0.0154, Val RMSE: 0.0250\n",
      "Epoch [118/200], Loss: 0.0005, Train RMSE: 0.0234, Val RMSE: 0.0220\n",
      "Epoch [119/200], Loss: 0.0002, Train RMSE: 0.0144, Val RMSE: 0.0259\n",
      "Epoch [120/200], Loss: 0.0002, Train RMSE: 0.0144, Val RMSE: 0.0210\n",
      "Epoch [121/200], Loss: 0.0003, Train RMSE: 0.0174, Val RMSE: 0.0224\n",
      "Epoch [122/200], Loss: 0.0003, Train RMSE: 0.0157, Val RMSE: 0.0212\n",
      "Epoch [123/200], Loss: 0.0004, Train RMSE: 0.0194, Val RMSE: 0.0199\n",
      "Epoch [124/200], Loss: 0.0006, Train RMSE: 0.0213, Val RMSE: 0.0246\n",
      "Epoch [125/200], Loss: 0.0010, Train RMSE: 0.0287, Val RMSE: 0.0213\n",
      "Epoch [126/200], Loss: 0.0005, Train RMSE: 0.0218, Val RMSE: 0.0244\n",
      "Epoch [127/200], Loss: 0.0002, Train RMSE: 0.0148, Val RMSE: 0.0355\n",
      "Epoch [128/200], Loss: 0.0009, Train RMSE: 0.0294, Val RMSE: 0.0274\n",
      "Epoch [129/200], Loss: 0.0020, Train RMSE: 0.0405, Val RMSE: 0.0297\n",
      "Epoch [130/200], Loss: 0.0007, Train RMSE: 0.0273, Val RMSE: 0.0407\n",
      "Epoch [131/200], Loss: 0.0011, Train RMSE: 0.0289, Val RMSE: 0.0426\n",
      "Epoch [132/200], Loss: 0.0015, Train RMSE: 0.0361, Val RMSE: 0.0231\n",
      "Epoch [133/200], Loss: 0.0009, Train RMSE: 0.0286, Val RMSE: 0.0276\n",
      "Epoch [134/200], Loss: 0.0004, Train RMSE: 0.0204, Val RMSE: 0.0273\n",
      "Epoch [135/200], Loss: 0.0006, Train RMSE: 0.0239, Val RMSE: 0.0290\n",
      "Epoch [136/200], Loss: 0.0005, Train RMSE: 0.0223, Val RMSE: 0.0250\n",
      "Epoch [137/200], Loss: 0.0005, Train RMSE: 0.0228, Val RMSE: 0.0265\n",
      "Epoch [138/200], Loss: 0.0004, Train RMSE: 0.0193, Val RMSE: 0.0236\n",
      "Epoch [139/200], Loss: 0.0003, Train RMSE: 0.0176, Val RMSE: 0.0222\n",
      "Epoch [140/200], Loss: 0.0003, Train RMSE: 0.0161, Val RMSE: 0.0193\n",
      "Epoch [141/200], Loss: 0.0001, Train RMSE: 0.0114, Val RMSE: 0.0240\n",
      "Epoch [142/200], Loss: 0.0004, Train RMSE: 0.0194, Val RMSE: 0.0218\n",
      "Epoch [143/200], Loss: 0.0006, Train RMSE: 0.0235, Val RMSE: 0.0320\n",
      "Epoch [144/200], Loss: 0.0007, Train RMSE: 0.0255, Val RMSE: 0.0220\n",
      "Epoch [145/200], Loss: 0.0005, Train RMSE: 0.0214, Val RMSE: 0.0253\n",
      "Epoch [146/200], Loss: 0.0004, Train RMSE: 0.0188, Val RMSE: 0.0310\n",
      "Epoch [147/200], Loss: 0.0011, Train RMSE: 0.0318, Val RMSE: 0.0278\n",
      "Epoch [148/200], Loss: 0.0016, Train RMSE: 0.0384, Val RMSE: 0.0248\n",
      "Epoch [149/200], Loss: 0.0007, Train RMSE: 0.0244, Val RMSE: 0.0312\n",
      "Epoch [150/200], Loss: 0.0005, Train RMSE: 0.0207, Val RMSE: 0.0321\n",
      "Epoch [151/200], Loss: 0.0010, Train RMSE: 0.0313, Val RMSE: 0.0228\n",
      "Epoch [152/200], Loss: 0.0008, Train RMSE: 0.0279, Val RMSE: 0.0325\n",
      "Epoch [153/200], Loss: 0.0008, Train RMSE: 0.0276, Val RMSE: 0.0349\n",
      "Epoch [154/200], Loss: 0.0014, Train RMSE: 0.0378, Val RMSE: 0.0435\n",
      "Epoch [155/200], Loss: 0.0024, Train RMSE: 0.0470, Val RMSE: 0.0289\n",
      "Epoch [156/200], Loss: 0.0020, Train RMSE: 0.0433, Val RMSE: 0.0294\n",
      "Epoch [157/200], Loss: 0.0014, Train RMSE: 0.0354, Val RMSE: 0.0354\n",
      "Epoch [158/200], Loss: 0.0014, Train RMSE: 0.0379, Val RMSE: 0.0292\n",
      "Epoch [159/200], Loss: 0.0009, Train RMSE: 0.0291, Val RMSE: 0.0289\n",
      "Epoch [160/200], Loss: 0.0009, Train RMSE: 0.0290, Val RMSE: 0.0219\n",
      "Epoch [161/200], Loss: 0.0006, Train RMSE: 0.0236, Val RMSE: 0.0283\n",
      "Epoch [162/200], Loss: 0.0004, Train RMSE: 0.0199, Val RMSE: 0.0228\n",
      "Epoch [163/200], Loss: 0.0004, Train RMSE: 0.0201, Val RMSE: 0.0214\n",
      "Epoch [164/200], Loss: 0.0005, Train RMSE: 0.0222, Val RMSE: 0.0239\n",
      "Epoch [165/200], Loss: 0.0004, Train RMSE: 0.0192, Val RMSE: 0.0218\n",
      "Epoch [166/200], Loss: 0.0009, Train RMSE: 0.0281, Val RMSE: 0.0258\n",
      "Epoch [167/200], Loss: 0.0011, Train RMSE: 0.0297, Val RMSE: 0.0264\n",
      "Epoch [168/200], Loss: 0.0004, Train RMSE: 0.0193, Val RMSE: 0.0268\n",
      "Epoch [169/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0281\n",
      "Epoch [170/200], Loss: 0.0006, Train RMSE: 0.0236, Val RMSE: 0.0236\n",
      "Epoch [171/200], Loss: 0.0005, Train RMSE: 0.0223, Val RMSE: 0.0268\n",
      "Epoch [172/200], Loss: 0.0003, Train RMSE: 0.0171, Val RMSE: 0.0261\n",
      "Epoch [173/200], Loss: 0.0005, Train RMSE: 0.0230, Val RMSE: 0.0290\n",
      "Epoch [174/200], Loss: 0.0004, Train RMSE: 0.0187, Val RMSE: 0.0225\n",
      "Epoch [175/200], Loss: 0.0003, Train RMSE: 0.0176, Val RMSE: 0.0230\n",
      "Epoch [176/200], Loss: 0.0003, Train RMSE: 0.0180, Val RMSE: 0.0259\n",
      "Epoch [177/200], Loss: 0.0004, Train RMSE: 0.0195, Val RMSE: 0.0252\n",
      "Epoch [178/200], Loss: 0.0005, Train RMSE: 0.0224, Val RMSE: 0.0237\n",
      "Epoch [179/200], Loss: 0.0004, Train RMSE: 0.0206, Val RMSE: 0.0214\n",
      "Epoch [180/200], Loss: 0.0002, Train RMSE: 0.0151, Val RMSE: 0.0267\n",
      "Epoch [181/200], Loss: 0.0003, Train RMSE: 0.0180, Val RMSE: 0.0242\n",
      "Epoch [182/200], Loss: 0.0002, Train RMSE: 0.0130, Val RMSE: 0.0316\n",
      "Epoch [183/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0186\n",
      "Epoch [184/200], Loss: 0.0002, Train RMSE: 0.0139, Val RMSE: 0.0216\n",
      "Epoch [185/200], Loss: 0.0002, Train RMSE: 0.0143, Val RMSE: 0.0229\n",
      "Epoch [186/200], Loss: 0.0003, Train RMSE: 0.0164, Val RMSE: 0.0234\n",
      "Epoch [187/200], Loss: 0.0005, Train RMSE: 0.0215, Val RMSE: 0.0212\n",
      "Epoch [188/200], Loss: 0.0002, Train RMSE: 0.0124, Val RMSE: 0.0225\n",
      "Epoch [189/200], Loss: 0.0003, Train RMSE: 0.0174, Val RMSE: 0.0287\n",
      "Epoch [190/200], Loss: 0.0008, Train RMSE: 0.0273, Val RMSE: 0.0230\n",
      "Epoch [191/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0267\n",
      "Epoch [192/200], Loss: 0.0008, Train RMSE: 0.0279, Val RMSE: 0.0218\n",
      "Epoch [193/200], Loss: 0.0005, Train RMSE: 0.0228, Val RMSE: 0.0214\n",
      "Epoch [194/200], Loss: 0.0004, Train RMSE: 0.0197, Val RMSE: 0.0212\n",
      "Epoch [195/200], Loss: 0.0006, Train RMSE: 0.0244, Val RMSE: 0.0208\n",
      "Epoch [196/200], Loss: 0.0003, Train RMSE: 0.0165, Val RMSE: 0.0293\n",
      "Epoch [197/200], Loss: 0.0005, Train RMSE: 0.0211, Val RMSE: 0.0230\n",
      "Epoch [198/200], Loss: 0.0006, Train RMSE: 0.0246, Val RMSE: 0.0227\n",
      "Epoch [199/200], Loss: 0.0006, Train RMSE: 0.0222, Val RMSE: 0.0212\n",
      "Epoch [200/200], Loss: 0.0003, Train RMSE: 0.0169, Val RMSE: 0.0260\n",
      "Finished Training\n",
      "Best model saved with Val RMSE: 0.0186\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define checkpoint path\n",
    "checkpoint_dir = os.path.join(data_dir, 'checkpoint')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop with accuracy tracking and checkpoint saving\n",
    "num_epochs = 200\n",
    "checkpoint_interval = 50  # Save checkpoint every 'n' epochs\n",
    "\n",
    "train_rmse_history = []\n",
    "val_rmse_history = []\n",
    "\n",
    "best_val_rmse = float('inf')  # Initialize best validation RMSE to a large value\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_rmse = 0.0\n",
    "    for inputs, labels, _ in train_loader:  # Assuming train_loader is your training DataLoader\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate RMSE for this batch\n",
    "        train_rmse += calculate_rmse(outputs, labels)\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    train_rmse /= len(train_loader)  # Average RMSE for the epoch\n",
    "    train_rmse_history.append(train_rmse)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_rmse = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in val_loader:  # Assuming val_loader is your validation DataLoader\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            val_rmse += calculate_rmse(outputs, labels)\n",
    "\n",
    "    val_rmse /= len(val_loader)  # Average RMSE for the epoch\n",
    "    val_rmse_history.append(val_rmse)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "          f'Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}')\n",
    "\n",
    "    # Save checkpoint if it is a checkpoint interval or the last epoch\n",
    "    if (epoch + 1) % checkpoint_interval == 0 or (epoch + 1) == num_epochs:\n",
    "        checkpoint_file = f'model_checkpoint_epoch_{epoch+1}.pth'\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, checkpoint_file)\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        #print(f'Saved checkpoint: {checkpoint_path}')\n",
    "\n",
    "    # Track the best validation RMSE and save the best model\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        #print(f'Best model saved with Val RMSE: {best_val_rmse:.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "print(f'Best model saved with Val RMSE: {best_val_rmse:.4f}')\n",
    "\n",
    "# Plot training and validation RMSE over epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_rmse_history, label='Train RMSE')\n",
    "plt.plot(range(1, num_epochs + 1), val_rmse_history, label='Val RMSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Train and Val RMSE Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# for 224x224 3-vector/9-vector and 30-img dataset, 200 epochs (10x faster on gpu than my mac, 5 min on GPU)...  barely depends on output dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Evaluate the model on val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[ 0.02489684 -0.10602148 -0.00076691  0.00839859 -0.19768363 -0.06827059\n",
      "  -0.02412415 -0.18339644 -0.1581826 ]], Ground Truth: [[-0.00024152 -0.10676479 -0.00238466 -0.00191188 -0.19126058 -0.05134499\n",
      "  -0.00449705 -0.18612087 -0.16756463]]\n",
      "Predicted: [[-0.01112173 -0.09439871  0.00373723  0.03161096 -0.17753646 -0.06774747\n",
      "   0.11502889 -0.15762372 -0.09282959]], Ground Truth: [[ 0.00327134 -0.10675478 -0.00221789  0.05606925 -0.17359805 -0.05000675\n",
      "   0.14250076 -0.15366375 -0.12532175]]\n",
      "Predicted: [[-0.00497432 -0.10046997  0.00487539 -0.03392935 -0.18975706 -0.03163688\n",
      "  -0.11910074 -0.16949762 -0.05346945]], Ground Truth: [[-0.00628877 -0.10679579 -0.00216877 -0.0756228  -0.17149305 -0.02554584\n",
      "  -0.18571019 -0.1668675  -0.0629555 ]]\n",
      "Predicted: [[-0.00239276 -0.09573945  0.00636625  0.08106974 -0.17225003 -0.02956617\n",
      "   0.20445515 -0.15645102 -0.05284077]], Ground Truth: [[ 0.00621653 -0.10673285 -0.00238931  0.07821894 -0.16644025 -0.03049803\n",
      "   0.18591416 -0.15246129 -0.07228839]]\n",
      "Val RMSE = 0.01749932044185698\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = os.path.join(data_dir, 'checkpoint')\n",
    "best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')\n",
    "\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "# testin on training set (should be able to memorize training data as a first step)\n",
    "val_rmse = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):  # Test on the first 5 images\n",
    "        inputs, labels, _ = val_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_rmse += calculate_rmse(outputs, labels)\n",
    "        print(f'Predicted: {outputs.cpu().numpy()}, Ground Truth: {labels.cpu().numpy()}')\n",
    "val_rmse /= len(val_dataset)\n",
    "print(f'Val RMSE = {val_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Test model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[-0.00668268 -0.10205215  0.00389377 -0.04078171 -0.1898047  -0.03928579\n",
      "  -0.11376605 -0.20404935 -0.09640968]], Ground Truth: [[-0.00239074 -0.10685873 -0.00206006 -0.05263531 -0.17902851 -0.04456234\n",
      "  -0.14287937 -0.18164945 -0.11797392]]\n",
      "Predicted: [[-0.00723765 -0.09880755  0.00536145 -0.05370172 -0.20343739 -0.0017317\n",
      "  -0.1598483  -0.21731584  0.01919955]], Ground Truth: [[-0.00903451 -0.1069802   0.00124073 -0.07653177 -0.17712235  0.00921786\n",
      "  -0.1891582  -0.20365357  0.02152622]]\n",
      "Predicted: [[-0.00645772 -0.09399348  0.00414283 -0.04135388 -0.18793972 -0.05670231\n",
      "  -0.094463   -0.18301703 -0.12388094]], Ground Truth: [[-0.00163829 -0.10683978 -0.00211477 -0.0343343  -0.18846679 -0.0446285\n",
      "  -0.10292017 -0.19354498 -0.13849175]]\n",
      "Predicted: [[-0.00264093 -0.09343958  0.00406022  0.08046491 -0.17587507 -0.02939214\n",
      "   0.19782424 -0.1733585  -0.05403538]], Ground Truth: [[ 0.00498879 -0.10690939 -0.00256503  0.05721009 -0.18488693 -0.02961791\n",
      "   0.15931404 -0.20022917 -0.08327913]]\n",
      "Test RMSE = 0.01566232251934707\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# testin on training set (should be able to memorize training data as a first step)\n",
    "test_rmse = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):  # Test on the first 5 images\n",
    "        inputs, labels, _ = test_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        test_rmse += calculate_rmse(outputs, labels)\n",
    "        print(f'Predicted: {outputs.cpu().numpy()}, Ground Truth: {labels.cpu().numpy()}')\n",
    "test_rmse /= len(test_dataset)\n",
    "print(f'Test RMSE = {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display gt vs predicted tip positions on image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'img_filename': 'sample_2.jpg', 'true_x': -0.004497051239013672, 'true_z': -0.16756463050842285, 'pred_x': -0.0241241455078125, 'pred_z': -0.15818260610103607}, {'img_filename': 'sample_5.jpg', 'true_x': 0.14250075817108154, 'true_z': -0.12532174587249756, 'pred_x': 0.11502888798713684, 'pred_z': -0.09282958507537842}, {'img_filename': 'sample_7.jpg', 'true_x': -0.18571019172668457, 'true_z': -0.06295549869537354, 'pred_x': -0.11910074204206467, 'pred_z': -0.0534694530069828}, {'img_filename': 'sample_13.jpg', 'true_x': 0.18591415882110596, 'true_z': -0.0722883939743042, 'pred_x': 0.2044551521539688, 'pred_z': -0.052840765565633774}]\n",
      "Plotted ground truth and prediction on sample_2.jpg and saved as data/rigidbodies/model_validation/val/output_sample_2.jpg.\n",
      "Plotted ground truth and prediction on sample_5.jpg and saved as data/rigidbodies/model_validation/val/output_sample_5.jpg.\n",
      "Plotted ground truth and prediction on sample_7.jpg and saved as data/rigidbodies/model_validation/val/output_sample_7.jpg.\n",
      "Plotted ground truth and prediction on sample_13.jpg and saved as data/rigidbodies/model_validation/val/output_sample_13.jpg.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Placeholder for collecting results\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        inputs, labels, img_filename = val_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        #print(labels)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Collect the ground truth and predictions\n",
    "        results.append({\n",
    "            'img_filename': img_filename, \n",
    "            'true_x': labels[0, 6].item(), #x3\n",
    "            'true_z': labels[0, 8].item(), #z3\n",
    "            'pred_x': outputs[0, 6].item(), #x3\n",
    "            'pred_z': outputs[0, 8].item() #z3\n",
    "        })\n",
    "    print(results)\n",
    "\n",
    "data_dir = 'data/rigidbodies'\n",
    "output_folder = 'val'\n",
    "dataset_file = os.path.join(data_dir, 'single_img_regression_single_rb_pos.csv')\n",
    "plot_predictions_on_images(results, data_dir, output_folder, dataset_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'img_filename': 'sample_15.jpg', 'true_x': -0.14287936687469482, 'true_z': -0.1179739236831665, 'pred_x': -0.11376605182886124, 'pred_z': -0.09640967845916748}, {'img_filename': 'sample_17.jpg', 'true_x': -0.18915820121765137, 'true_z': 0.021526217460632324, 'pred_x': -0.15984830260276794, 'pred_z': 0.01919955015182495}, {'img_filename': 'sample_23.jpg', 'true_x': -0.10292017459869385, 'true_z': -0.13849174976348877, 'pred_x': -0.09446299821138382, 'pred_z': -0.12388093769550323}, {'img_filename': 'sample_27.jpg', 'true_x': 0.15931403636932373, 'true_z': -0.08327913284301758, 'pred_x': 0.19782423973083496, 'pred_z': -0.054035384207963943}]\n",
      "Plotted ground truth and prediction on sample_15.jpg and saved as data/rigidbodies/model_validation/test/output_sample_15.jpg.\n",
      "Plotted ground truth and prediction on sample_17.jpg and saved as data/rigidbodies/model_validation/test/output_sample_17.jpg.\n",
      "Plotted ground truth and prediction on sample_23.jpg and saved as data/rigidbodies/model_validation/test/output_sample_23.jpg.\n",
      "Plotted ground truth and prediction on sample_27.jpg and saved as data/rigidbodies/model_validation/test/output_sample_27.jpg.\n"
     ]
    }
   ],
   "source": [
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Placeholder for collecting results\n",
    "results = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):\n",
    "        inputs, labels, img_filename = test_dataset[i]\n",
    "        inputs = inputs.unsqueeze(0).to(device)  # Add batch dimension and move to GPU\n",
    "        labels = labels.unsqueeze(0).to(device)\n",
    "        #print(labels)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Collect the ground truth and predictions\n",
    "        results.append({\n",
    "            'img_filename': img_filename, \n",
    "            'true_x': labels[0, 6].item(),\n",
    "            'true_z': labels[0, 8].item(),\n",
    "            'pred_x': outputs[0, 6].item(),\n",
    "            'pred_z': outputs[0, 8].item()\n",
    "        })\n",
    "    print(results)\n",
    "\n",
    "\n",
    "data_dir = 'data/rigidbodies'\n",
    "output_folder = 'test'\n",
    "dataset_file = os.path.join(data_dir, 'single_img_regression_single_rb_pos.csv')\n",
    "plot_predictions_on_images(results, data_dir, output_folder, dataset_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time on CPU: 0.035125 seconds\n",
      "Average inference time on GPU: 0.002542 seconds\n",
      "gpu is 13 times faster than cpu\n",
      "cpu max inference rate = 28 Hz\n",
      "gpu max inference rate = 393 Hz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from math import floor\n",
    "\n",
    "# Assume `model` is your already trained model\n",
    "\n",
    "# Single image tensor (assuming it is already preprocessed)\n",
    "input_image = torch.randn(1, 3, 224, 224)  # Batch size of 1, 3 channels (RGB), 224x224 resolution\n",
    "\n",
    "# Ensure that the model is correctly loaded on the CPU\n",
    "model_cpu = model.to('cpu')\n",
    "model_cpu.eval()\n",
    "\n",
    "# Measure inference time on CPU\n",
    "input_image_cpu = input_image.to('cpu')  # Ensure the input tensor is on the CPU\n",
    "with torch.no_grad():\n",
    "    start_time = time.time()\n",
    "    for _ in range(100):  # Run inference 100 times to get an average\n",
    "        _ = model_cpu(input_image_cpu)\n",
    "    end_time = time.time()\n",
    "    avg_cpu_time = (end_time - start_time) / 100\n",
    "    print(f'Average inference time on CPU: {avg_cpu_time:.6f} seconds')\n",
    "\n",
    "# Measure inference time on GPU (if available)\n",
    "if torch.cuda.is_available():\n",
    "    model_gpu = model.to('cuda')\n",
    "    model_gpu.eval()\n",
    "    input_image_gpu = input_image.to('cuda')  # Ensure the input tensor is on the GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Warm-up GPU (GPU needs a warm-up time for the first operation)\n",
    "        _ = model_gpu(input_image_gpu)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for _ in range(100):  # Run inference 100 times to get an average\n",
    "            _ = model_gpu(input_image_gpu)\n",
    "        # Wait for GPU to finish (synchronize)\n",
    "        torch.cuda.synchronize()\n",
    "        end_time = time.time()\n",
    "        avg_gpu_time = (end_time - start_time) / 100\n",
    "        print(f'Average inference time on GPU: {avg_gpu_time:.6f} seconds')\n",
    "\n",
    "cpu_over_gpu = avg_cpu_time / avg_gpu_time\n",
    "print(f'gpu is {int(cpu_over_gpu)} times faster than cpu')\n",
    "\n",
    "cpu_hz = 1/avg_cpu_time\n",
    "gpu_hz = 1/avg_gpu_time\n",
    "print(f'cpu max inference rate = {floor(cpu_hz)} Hz')\n",
    "print(f'gpu max inference rate = {floor(gpu_hz)} Hz')\n",
    "\n",
    "# on ceres:\n",
    "# Average inference time on CPU: 0.035125 seconds\n",
    "# Average inference time on GPU: 0.002542 seconds\n",
    "# gpu is 13 times faster than cpu\n",
    "# cpu max inference rate = 28 Hz\n",
    "# gpu max inference rate = 393 Hz\n",
    "# but this varies from call to call\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
